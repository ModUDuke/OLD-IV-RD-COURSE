---
title: 'Instrumental Variables in Practice'
description: 'This chapter covers more details about Instrumental Variables Analysis, and allows you to practice instrumental variables for yourself'
---

## The Problem of Weak Instruments

```yaml
type: VideoExercise
key: bf3c9abc64
lang: r
xp: 50
skills: 1
video_link: //player.vimeo.com/video/219559996
```


---

## An Example of a Weak Instrument

```yaml
type: MultipleChoiceExercise
key: 02e185272f
lang: r
xp: 50
skills: 1
```

A public health specialist names Phoebe is interested in how a person's wealth (their assets minus their debts) might effect their health. Phoebe has longitudinal data on people's wealth and health, but she's having trouble determining whether wealth has a causal effect on health with traditional regression models. Phoebe's concern is that a person's health varies endogenously with their weath; a person's wealth might have a positive causal effect on their health, but a person's health could also have a positive causal effect on their wealth. 

To untangle this causal relationship, Phoebe decides to conduct an instrumental variable analysis of wealth's effect on health with inheritance as an exogenous instrument of wealth (that is, inheritance effects a person's wealth but does not have any direct or indirect relationship with a person's health other than through how it might affect wealth). However, Phoebe's analysis yields no statistically significant results. Phoebe also tests the relationship between inheritance and wealth, and finds that there is only a very weak correlation. Which of the following could this imply?    
  
  *** =instructions
- Inheritance has a direct causal effect on health.
- These results are uninformative about any potential causality between wealth and health.
- People who inherit money often feel guilty, leading to a decline in health.
- People are more likely to inherit money when they themselves have poor health.

`@possible_answers`


`@hint`


`@pre_exercise_code`
```{r}

```

`@sct`
```{r}
msg1 = "Although this instrumental variable would be poor for this analysis (an instrument should not have a direct effect on the dependent variable), this is not an example of a 'weak' instrumental variable."
msg2 = "Correct! Only a small percentage of people ever inherit a substantial amount of money, and those who do tend to have better wealth to begin with. So despite our theories, the data we have might have very little statistical power on the relationship between wealth and health."
msg3 = "This answer is very similar to the first one. In this case, the problem is not that the instrument is 'weak' but that the instrument has a direct causal effect on health."
msg4 = "Although this instrumental variable would be poor for this analysis (an instrument should be randomly assigned), this is not an example of a 'weak' instrumental variable."

test_mc(correct = 2, feedback_msgs = c(msg1,msg2,msg3,msg4))
```

---

## Overview of Research on Property Rights & Economic Development

```yaml
type: VideoExercise
key: 7b60f7d7c0
lang: r
xp: 50
skills: 1
video_link: //player.vimeo.com/video/305800702
```


---

## Where Do Instruments Come From?

```yaml
type: VideoExercise
key: 90f146a86a
lang: r
xp: 50
skills: 1
video_link: //player.vimeo.com/video/219560424
```


---

## Using IV When There is Measurement Error

```yaml
type: MultipleChoiceExercise
key: 76ab8a34c9
lang: r
xp: 50
skills: 1
```

You're doing some research and plan on using an instrumental variables strategy to help with any noncompliance. But your instrument was hard to measure perfectly, so you think its values have some measurement error. Two of the following statements are valid for an IV analysis, but which one would violate the assumptions of IV analysis and make our results invalid?

`@possible_answers`
- The instrument's measurement error is independent of the values for the treatment and outcome variables
- The instrument's measurement error is independent of the values of the treatment variable, but not for the outcome variable
- The instrument's measurement error is independent of the values from the outcome variable, but not for the treatment variable

`@hint`


`@pre_exercise_code`
```{r}

```

`@sct`
```{r}
msg1 = "This is perfectly fine with IV analysis. If the measurement error is not affecting the measurement of the treatment and outcome variables, then we're safe. We are looking for the option that would violate the IV assumptions, so try again."
msg2 = "Correct! This would break our IV analysis, because it would mean that the instrument has some direct effect on the values of theoutcome variable by itself, but to use IV correctly, we need the instrument to only indirectly act on the outcome via the treatment variable."
msg3 = "This is perfectly fine with IV analysis. We expect the instrument to have a causal effect on the treatment variable, so it would make sense that the measurement error would show up in the value of our treatment variable. But we are looking for the option that would violate the IV assumptions, so try again."
test_mc(correct = 2, feedback_msgs = c(msg1,msg2,msg3))
```

---

## Logical Arguments About Instrumental Variables

```yaml
type: MultipleChoiceExercise
key: f9981cd29c
lang: r
xp: 50
skills: 1
```

Let's say that you work for the city government of a town that has been rated as having the least happy children in the country. Your town also wants to increase the number of bike lanes on city streets. As a creative data analyst, you think that bike lanes may encourage families to exercise together, and exercising as a family may change the unhappy kids in your town into happy kids. You get your hands on a national dataset that has statistics on bike lanes, family exercise, and child happiness from many cities. You think that you can use bike lanes as an instrumental variable to see if family exercise has an effect on child happiness. Your basic argument is that bike lanes cause families to exercise together, and family exercise is correlated with child happiness, so bike lanes are an instrument for the effect of family exercise on child happiness. What is your best argument that the instrument does not have a direct causal effect on the outcome variable? 
  
  *** =instructions
- Kids are not made happier from the painted lines on the road
- All kids are equally happy when they are on a bike, whether alone or with their family
- Traffic safety with bike lanes always varies, but kids adjust as needed
- Some kids are happier to ride in bike lanes and happier to bike offroad, but those preferences cancel each other out

`@possible_answers`


`@hint`


`@pre_exercise_code`
```{r}

```

`@sct`
```{r}
msg1 = "Correct! This is the best argument because, if kids only bike when there are bike lanes, then this argument would suddenly become a very weak one, because simply painting new lines on a road would make kids very happy, independently of whether they actually ride a bike or not."
msg2 = "This might be true, but that is directed more at whether the treatment (riding with a family) affects the outcome (child happiness), not the instrument on the outcome like the question asks. Try again"
msg3 = "Kids may need to feel safe to be happy, but we aren't trying to use a 'feeling of safety' as an instrument. Try again"
msg4 = "If this were true, it sounds like the instrument would be uncorrelated with our outcome, which would make our IV analysis invalid, because we need the instrument to be correlated to the outcome exlcusively by the causal effect of the treatment. So try again."
test_mc(correct = 1, feedback_msgs = c(msg1,msg2,msg3,msg4))
```

---

## Let's Code: Practice Using Instrumental Variables

```yaml
type: VideoExercise
key: f0833c4db3
lang: r
xp: 50
skills: 1
video_link: //player.vimeo.com/video/306239731
```


---

## Practice Using Instrumental Variables: CreditCo

```yaml
type: NormalExercise
key: 80186483d8
lang: r
xp: 100
skills: 1
```

If you took our Experiments course, you assessed the validity of a natural experiment using data from CreditCo. Let's revisit that analysis, framing it in the logic of instrumental variables.

As you may recall, CreditCo sent out offers in the mail to increase their customers' credit limits. We are worried that the customers who took up the offer might not have randomly done so, which causes an endogeneity problem. We proposed to solve this problem by using the rainy weather on the day the credit offers were delivered as an instrument for treatment. Specifically, you realized that heavy storms across the country hit half the zipcodes in the treatment group, while the other half of customers experienced sunny weather the day the offer arrived. You realize this variation in the weather may have made customers in the rainy zones feel depressed and therefore ignored the offer, while those in the sunny zones felt cheerful, and because of their good mood took up the offer. Because the weather is out of anyone's control, we can argue it is exogenous to any other factors related to credit card offers, and it also seems to be the exclusive source of variation between the treatment and control groups. Let's try using IV analysis to see if it provides a convincing causal explanation.

Included on the workspace is a dataset from CreditCo. The data are a sample of CreditCo customers who were offered the credit limit increase. We will focus on the variable `rainy`, which we will use as an instrument for `opt_in`, which we suspect is endogenous to our outcomes of interest.

`@instructions`
- 1) Explore the dataset `CreditCo` on your own with basic tools like head(), str(), and summary().
- 2) Using a linear regression, compute the correlation between opt-in and rain.
- 3) Test whether rain satisfies the relevance assumption (i.e. if its correlation with opt-in is significantly different from zero).
- 4) What is the sign of the correlation between rain and opt-in?

`@hint`
- Remember to be aware of selecting the correct subsample

`@pre_exercise_code`
```{r}
  set.seed(1)
  n             <- 9e5
  frac_treated  <- .5
  frac_female   <- .5656
  frac_white    <- .688

# Initialize dataframe
  CreditCo <- as.data.frame(matrix(0, ncol=11,nrow=n))
  colnames(CreditCo) <- c("id","offered","opt_in","FICO","age","female","race_white","default_pre","default_post","balance_pre","balance_post")

# Simulate baseline data
  CreditCo$id         <- seq(1,n,1)
  CreditCo$offered    <- as.integer(runif(n)<frac_treated)
  CreditCo$opt_in     <- rep.int(0,n)
  CreditCo$FICO       <- rnorm(n, mean=736, sd=300)
  CreditCo$age        <- sample(18:55, n, replace=T)
  CreditCo$female     <- as.integer(runif(n)<frac_female)
  CreditCo$race_white <- as.integer(runif(n)<frac_white)

# make FICO score intelligible
  CreditCo$FICO[CreditCo$FICO>850] <- 850
  CreditCo$FICO[CreditCo$FICO<300] <- 300
  CreditCo$FICO                    <- round(CreditCo$FICO)

# simulate pre-experiment default rate
  draw <- runif(n)
  xb   <- -1.82-0.2*CreditCo$FICO/100+.046*(CreditCo$FICO^2)/10000-5.1*CreditCo$female-7*CreditCo$race_white
  p    <- exp(xb)/(1+exp(xb))
  CreditCo$default_pre <- as.integer(draw<p)

# simulate pre-experiment balance level
  draw <- rnorm(n, mean=0, sd=0.5)
  CreditCo$balance_pre <- 8.5-0.2*CreditCo$FICO/100+.046*(CreditCo$FICO^2)/10000-0.15*CreditCo$female-0.85*CreditCo$race_white + draw
  CreditCo$balance_pre <- exp(CreditCo$balance_pre)

# Simulate opt-in behavior based on weather
  draw <- runif(n)
  draw2 <- runif(n)
  CreditCo$rainy <- draw>0.5
  xb   <- 1.5-3*CreditCo$rainy+.2*CreditCo$female
  p    <- exp(xb)/(1+exp(xb))
  CreditCo$opt_in[CreditCo$offered==1] <- as.integer(draw2[CreditCo$offered==1]<p[CreditCo$offered==1])

# Simulate post outcomes
  draw <- runif(n)
  xb   <- -1.82-0.2*CreditCo$FICO/100+.046*(CreditCo$FICO^2)/10000-5.1*CreditCo$female-7*CreditCo$race_white+4*CreditCo$offered*CreditCo$opt_in
  p    <- exp(xb)/(1+exp(xb))
  CreditCo$default_post <- as.integer(draw<p)

# balance
  draw <- rnorm(n, mean=0, sd=0.5)
  CreditCo$balance_post <- 8.5-0.2*CreditCo$FICO/100+.046*(CreditCo$FICO^2)/10000-0.15*CreditCo$female-0.85*CreditCo$race_white + 1*CreditCo$offered*CreditCo$opt_in + draw
  CreditCo$balance_post <- exp(CreditCo$balance_post)

# Remove elements and variables from environment
  rm(draw,frac_female,frac_treated,frac_white,n,p,xb)
  CreditCo <- CreditCo[CreditCo$offered==1,c("id","opt_in","FICO","female","race_white","default_pre","default_post","balance_pre","balance_post","rainy")]
```

`@sample_code`
```{r}
# The dataset `CreditCo` is available in your workspace. Here is a data dictionary to help you make sense of the variable names and what they are representing:

*Data Dictionary*

#  `id` - customer ID
#  `offered` - did customer get the offer for a higher limit?
#  `opt_in` - did the customer opt into the higher limit?
#  `FICO` - an industry standard credit rating score per customer
#  `age` - customer age
#  `female` - 1 if gender is female
#  `race_white` -  1 if race is recorded as white, 0 if anything else
#  `default_pre` - customer's default rate before the offer
#  `default_post` - customer's default rate after the offer
#  `balance_pre` - customer's credit balance before the offer
#  `balance_post` - customer's credit balance after the offer
#  `rainy` - TRUE if it rained at the customer's house


# 1) On your own, explore the dataframe and get familiar with the ranges of values and data types it contains. We suggest using commands like head(), str(), and summary() for different variables to get a sense of what the data look like. 




# 2) First we need to determine whether our instrument satisfies the Relevance Assumption, and we'll start this process by looking to see if it has any correlation with our outcome variable. Using a linear regression, compute the correlation coefficient between rain and opt-in status, and then run the summary() command to see if there is a correlation between the instrument and our treatment.

    Solution2 <- lm( ~ ,data=CreditCo)
    summary()

# 3) If that regression does show a correlation between opt-in status and rainy weather, we need to make sure it's a statistically significant one. What is the p-value associated with the t-test on the coefficient of rain? Enter in the p-value for this t-test as Solution3:

    Solution3 <-

# 4) What is the sign of the correlation between rain and opt-in? You can write "positive", "negative", or "no correlation" into Solution4 as the answer:

    Solution4 <- ""


```

`@solution`
```{r}
Solution2 <- lm(opt_in~rainy,data=CreditCo)
  summary(Solution2)
Solution3 <- summary(Solution2)$coefficients[2,4]
Solution4 <- "negative"
```

`@sct`
```{r}
test_object("Solution2")
test_object("Solution3")
test_object("Solution4")
success_msg("Good work! Our instrument does seem to have a statistically significant negative correlation with our outcome variable. This is one important pillar that supports our argument that we can use the rainy weather as an instrument in this analysis.")
```

---

## Refutability and Nonrefutability of the Instrumental Variables Assumptions

```yaml
type: VideoExercise
key: 9cc4cfa847
lang: r
xp: 50
skills: 1
video_link: //player.vimeo.com/video/219559377
```


---

## Questioning the Validity of Instrumental Variables Results

```yaml
type: MultipleChoiceExercise
key: db21106fcb
lang: r
xp: 50
skills: 1
```

Remember back to an earlier video, where we looked at the hyothetical question of whether going to college increases how much money you make, using the travel distance between home and the college as an instrumental variable. Researchers looking at this analysis will be worrying about the nonrefutable Exclusion Restriction and Exogeneity Assumption in this design. What do you think the most valid criticism would be?

`@possible_answers`
- Researchers think the Relevance Assumption does not hold: whether you live close to a college or not doesn't actually affect whether you go to college or not
- Researchers think the Exclusion Restriction does not hold: employers are willing to pay people more money if students went to high school in a town that was close to a college
- Researchers think the Exogeneity Assumption does not hold: there are unobserved variables, like ability, which are correlated with distance-to-college and which also affect how much income you'll make

`@hint`


`@pre_exercise_code`
```{r}

```

`@sct`
```{r}
msg1 = "Not quite. Remember that relevance is refutable, so people don't usually complain about this because it can be checked in the data. Try again"
msg2 = "This is probably not the most common complaint, because it would take a long and complex argument to justify this as the single main reason why the IV analysis is invalid. Try again"
msg3 = "Correct! This would be the most common complaint, and it's a good one. Luckily, the example was theoretical!"
test_mc(correct = 3, feedback_msgs = c(msg1,msg2,msg3))
```

---

## Indirect Inference

```yaml
type: VideoExercise
key: 88c5d08f7e
lang: r
xp: 50
skills: 1
video_link: //player.vimeo.com/video/219559540
```


---

## Let's Code: Practice Computing Causal Effects Using Indirect Inference

```yaml
type: VideoExercise
key: 26d480f5ba
lang: r
xp: 50
skills: 1
video_link: //player.vimeo.com/video/306239739
```


---

## Practice Computing Causal Effects Using Indirect Inference: CreditCo

```yaml
type: NormalExercise
key: 721d6bc016
lang: r
xp: 100
skills: 1
```

Now let's repeat the previous CreditCo analysis, but now use indirect inference to compute the causal effect of opting in to a credit limit increase on one's credit balance.

The setting is the same as before. CreditCo sent out offers in the mail to increase their customers' credit limits. But we know that taking up the offer is not randomly determined. We proposed to solve this problem by using the whether it rained on the day the credit offers were delivered as an instrument for takeup. 

In this exercise, we will restrict our analysis to the set of customers who received the offer. We will use the method of indirect inference to estimate the causal effect of takeup on credit balance.

Included on the workspace is a data set from CreditCo. The data are a sample of CreditCo customers who were offered the credit limit increase. We will focus on the variable `rainy`, which we will use as an instrument for `opt_in`, which we suspect is endogenous to our outcomes of interest.

`@instructions`
- 1) Using a linear regression, compute the reduced-form effect (i.e. correlation between credit balance and rain)
- 2) Using a linear regression, compute the first-stage effect (i.e. correlation between opting in and rain)
- 3) Now compute the causal effect, which is equal to (reduced form)/(first stage)
- 4) Did opting in to the credit offer increase or decrease credit balances?

`@hint`
- Remember to be aware of selecting the correct subsample

`@pre_exercise_code`
```{r}
  set.seed(1)
  n             <- 9e5
  frac_treated  <- .5
  frac_female   <- .5656
  frac_white    <- .688

# Initialize dataframe
  CreditCo <- as.data.frame(matrix(0, ncol=11,nrow=n))
  colnames(CreditCo) <- c("id","offered","opt_in","FICO","age","female","race_white","default_pre","default_post","balance_pre","balance_post")

# Simulate baseline data
  CreditCo$id         <- seq(1,n,1)
  CreditCo$offered    <- as.integer(runif(n)<frac_treated)
  CreditCo$opt_in     <- rep.int(0,n)
  CreditCo$FICO       <- rnorm(n, mean=736, sd=300)
  CreditCo$age        <- sample(18:55, n, replace=T)
  CreditCo$female     <- as.integer(runif(n)<frac_female)
  CreditCo$race_white <- as.integer(runif(n)<frac_white)

# make FICO score intelligible
  CreditCo$FICO[CreditCo$FICO>850] <- 850
  CreditCo$FICO[CreditCo$FICO<300] <- 300
  CreditCo$FICO                    <- round(CreditCo$FICO)

# simulate pre-experiment default rate
  draw <- runif(n)
  xb   <- -1.82-0.2*CreditCo$FICO/100+.046*(CreditCo$FICO^2)/10000-5.1*CreditCo$female-7*CreditCo$race_white
  p    <- exp(xb)/(1+exp(xb))
  CreditCo$default_pre <- as.integer(draw<p)

# simulate pre-experiment balance level
  draw <- rnorm(n, mean=0, sd=0.5)
  CreditCo$balance_pre <- 8.5-0.2*CreditCo$FICO/100+.046*(CreditCo$FICO^2)/10000-0.15*CreditCo$female-0.85*CreditCo$race_white + draw
  CreditCo$balance_pre <- exp(CreditCo$balance_pre)

# Simulate opt-in behavior based on weather
  draw <- runif(n)
  draw2 <- runif(n)
  CreditCo$rainy <- draw>0.5
  xb   <- 1.5-3*CreditCo$rainy+.2*CreditCo$female
  p    <- exp(xb)/(1+exp(xb))
  CreditCo$opt_in[CreditCo$offered==1] <- as.integer(draw2[CreditCo$offered==1]<p[CreditCo$offered==1])

# Simulate post outcomes
  draw <- runif(n)
  xb   <- -1.82-0.2*CreditCo$FICO/100+.046*(CreditCo$FICO^2)/10000-5.1*CreditCo$female-7*CreditCo$race_white+4*CreditCo$offered*CreditCo$opt_in
  p    <- exp(xb)/(1+exp(xb))
  CreditCo$default_post <- as.integer(draw<p)

# balance
  draw <- rnorm(n, mean=0, sd=0.5)
  CreditCo$balance_post <- 8.5-0.2*CreditCo$FICO/100+.046*(CreditCo$FICO^2)/10000-0.15*CreditCo$female-0.85*CreditCo$race_white + 1*CreditCo$offered*CreditCo$opt_in + draw
  CreditCo$balance_post <- exp(CreditCo$balance_post)

# Remove elements and variables from environment
  rm(draw,frac_female,frac_treated,frac_white,n,p,xb)
  CreditCo <- CreditCo[CreditCo$offered==1,c("id","opt_in","balance_post","rainy")]
```

`@sample_code`
```{r}
# The dataset `CreditCo` is available in your workspace. Again, here is a data dictionary to help you make sense of the variable names and what they are representing:

*Data Dictionary*

#  `id` - customer ID
#  `offered` - did customer get the offer for a higher limit?
#  `opt_in` - did the customer opt into the higher limit?
#  `FICO` - an industry standard credit rating score per customer
#  `age` - customer age
#  `female` - 1 if gender is female
#  `race_white` -  1 if race is recorded as white, 0 if anything else
#  `default_pre` - customer's default rate before the offer
#  `default_post` - customer's default rate after the offer
#  `balance_pre` - customer's credit balance before the offer
#  `balance_post` - customer's credit balance after the offer
#  `rainy` - TRUE if it rained at the customer's house
#  `coefficients` - where we will store coefficients from our regressions


# In order to see if a higher credit limit will make people buy more things with their credit cards, we  will use indirect inference to calculate the causal effect of getting a credit card offer on the credit balances of the customers. This will take 3 steps: first, we will calculate the correlation between our instrument and our outcome variable, then we will calculate the correlation between our instrument and our treatment, and then we'll find the ratio between these correlations.

# 1) Using a linear regression, compute the reduced-form effect (i.e. correlation between `balance_post` and `rainy`),and then run the summary() command to look at the correlation coefficient between the outcome and the instrument.

    Solution1<-lm( ~ ,data=CreditCo)
    summary()

# 2) Using a linear regression, compute the first-stage effect (i.e. correlation between `opt_in` and `rainy`), and then run the summary() command to look at the correlation coefficient between the treatment and the instrument.

    Solution2<-lm( ~ ,data=CreditCo)
    summary()

# 3) Compute the causal effect, which is equal to (reduced form)/(first stage), then look at the result :

    Solution3<-Solution1$coefficients[2]/Solution2$coefficients[2]
    Solution3

# 4) Did opting in to the credit offer make customer's credit balances increase or decrease? (type "increase" if Solution3 is positive or "decrease" if Solution3 is negative)

    Solution4<-""


```

`@solution`
```{r}
Solution1 <- lm(balance_post~rainy,data=CreditCo)
  summary(Solution1)
Solution2 <- lm(opt_in~rainy,data=CreditCo)
  summary(Solution2)
Solution3 <- Solution1$coefficients[2]/Solution2$coefficients[2]
Solution4 <- "increase"
```

`@sct`
```{r}
test_object("Solution1")
test_object("Solution2")
test_object("Solution3")
test_object("Solution4")
success_msg("Good work! Based on this indirect inference analysis, the offer of a higher credit limit does indeed seem to cause people buy more stuff on their credit card. But let's see how this looks with the 2SLS method as well!")
```

---

## Two Stage Least Squares (2SLS)

```yaml
type: VideoExercise
key: 91d63315da
lang: r
xp: 50
skills: 1
video_link: //player.vimeo.com/video/219559617
```


---

## Overview of Squatters & Property Rights Experiment

```yaml
type: VideoExercise
key: 3c7eb6ff7d
lang: r
xp: 50
skills: 1
video_link: //player.vimeo.com/video/304168081
```


---

## Example of IV: Property Rights & Market Beliefs

```yaml
type: VideoExercise
key: 43960fe668
lang: r
xp: 50
skills: 1
video_link: //player.vimeo.com/video/219560477
```


---

## Let's Code: Practice Computing Causal Effects Using 2SLS

```yaml
type: VideoExercise
key: 324a3a8f49
lang: r
xp: 50
skills: 1
video_link: //player.vimeo.com/video/306239751
```


---

## Practice Computing Causal Effects Using Two-Stage Least Squares (2SLS): CreditCo

```yaml
type: NormalExercise
key: 415cb190e2
lang: r
xp: 100
skills: 1
```

Now let us repeat the previous CreditCo analysis, but now use 2SLS to compute the causal effect instead of indirect inference.

`@instructions`
- 1) Using a linear regression, compute the first-stage effect (i.e. correlation between opting in and rain)
- 2) Compute the predicted values of opting in
- 3) Compute the causal effect by linear regression (i.e. correlation between credit balance and predicted opt-in rate)
- 4) Did opting in to the credit offer increase or decrease credit balances?

`@hint`
- Remember to be aware of selecting the correct subsample

`@pre_exercise_code`
```{r}
  set.seed(1)
  n             <- 9e5
  frac_treated  <- .5
  frac_female   <- .5656
  frac_white    <- .688

# Initialize dataframe
  CreditCo <- as.data.frame(matrix(0, ncol=11,nrow=n))
  colnames(CreditCo) <- c("id","offered","opt_in","FICO","age","female","race_white","default_pre","default_post","balance_pre","balance_post")

# Simulate baseline data
  CreditCo$id         <- seq(1,n,1)
  CreditCo$offered    <- as.integer(runif(n)<frac_treated)
  CreditCo$opt_in     <- rep.int(0,n)
  CreditCo$FICO       <- rnorm(n, mean=736, sd=300)
  CreditCo$age        <- sample(18:55, n, replace=T)
  CreditCo$female     <- as.integer(runif(n)<frac_female)
  CreditCo$race_white <- as.integer(runif(n)<frac_white)

# make FICO score intelligible
  CreditCo$FICO[CreditCo$FICO>850] <- 850
  CreditCo$FICO[CreditCo$FICO<300] <- 300
  CreditCo$FICO                    <- round(CreditCo$FICO)

# simulate pre-experiment default rate
  draw <- runif(n)
  xb   <- -1.82-0.2*CreditCo$FICO/100+.046*(CreditCo$FICO^2)/10000-5.1*CreditCo$female-7*CreditCo$race_white
  p    <- exp(xb)/(1+exp(xb))
  CreditCo$default_pre <- as.integer(draw<p)

# simulate pre-experiment balance level
  draw <- rnorm(n, mean=0, sd=0.5)
  CreditCo$balance_pre <- 8.5-0.2*CreditCo$FICO/100+.046*(CreditCo$FICO^2)/10000-0.15*CreditCo$female-0.85*CreditCo$race_white + draw
  CreditCo$balance_pre <- exp(CreditCo$balance_pre)

# Simulate opt-in behavior based on weather
  draw <- runif(n)
  draw2 <- runif(n)
  CreditCo$rainy <- draw>0.5
  xb   <- 1.5-3*CreditCo$rainy+.2*CreditCo$female
  p    <- exp(xb)/(1+exp(xb))
  CreditCo$opt_in[CreditCo$offered==1] <- as.integer(draw2[CreditCo$offered==1]<p[CreditCo$offered==1])

# Simulate post outcomes
  draw <- runif(n)
  xb   <- -1.82-0.2*CreditCo$FICO/100+.046*(CreditCo$FICO^2)/10000-5.1*CreditCo$female-7*CreditCo$race_white+4*CreditCo$offered*CreditCo$opt_in
  p    <- exp(xb)/(1+exp(xb))
  CreditCo$default_post <- as.integer(draw<p)

# balance
  draw <- rnorm(n, mean=0, sd=0.5)
  CreditCo$balance_post <- 8.5-0.2*CreditCo$FICO/100+.046*(CreditCo$FICO^2)/10000-0.15*CreditCo$female-0.85*CreditCo$race_white + 1*CreditCo$offered*CreditCo$opt_in + draw
  CreditCo$balance_post <- exp(CreditCo$balance_post)

# Remove elements and variables from environment
  rm(draw,frac_female,frac_treated,frac_white,n,p,xb)
  CreditCo <- CreditCo[CreditCo$offered==1,c("id","opt_in","balance_post","rainy")]
```

`@sample_code`
```{r}
# The dataset `CreditCo` is available in your workspace. Onc again, here is a data dictionary to help you make sense of the variable names and what they are representing:

*Data Dictionary*

#  `id` - customer ID
#  `offered` - did customer get the offer for a higher limit?
#  `opt_in` - did the customer opt into the higher limit?
#  `FICO` - an industry standard credit rating score per customer
#  `age` - customer age
#  `female` - 1 if gender is female
#  `race_white` -  1 if race is recorded as white, 0 if anything else
#  `default_pre` - customer's default rate before the offer
#  `default_post` - customer's default rate after the offer
#  `balance_pre` - customer's credit balance before the offer
#  `balance_post` - customer's credit balance after the offer
#  `rainy` - TRUE if it rained at the customer's house


# We'll use the 2-Stage Least Squares method to calculate our causal effect through our instrumental variable, `rainy`. The first stage is to find the correlation between the treatment and the instrument, which we will store in the variable `prediction`, and the second stage is to find the correlation between our outcome of interest, `balance_post`, and our adjusted treatment variable `prediction`. This will give us the causal effect of being offered a higher credit limit on our outcome of interest, the credit card balance of CreditCo customers.

# 1) Using a linear regression, compute the first-stage effect (i.e. correlation between `opt_in` and `rainy`), and look at the results with the summary() function:

    Solution1<-lm( ~ ,data=CreditCo)
    summary()

# 2) We will now take this correlation between our instrument and our treatment and generate a predicted opt-in rate for all of the customers in our database. Compute the predicted opt-in rate for each person, and add them to our dataset through the variable `prediction` in the CreditCo dataframe:

    Solution2<-predict(Solution1,data=CreditCo)
    CreditCo$prediction <- Solution2

# 3) Now let's compute our second-stage effect, which is the correlation between our predicted treatment values and our outcome of interest. To do this, compute the causal effect, which is a regression of credit balance on the predicted treatment values:

    Solution3<-lm( ~ ,data=CreditCo)
    summary()

# 4) Based on our use of the instrumental variable of `rainy`, did opting in to the credit offer increase or decrease credit balances of CreditCo's customers? (to Solution 4, type "increased" or "decreased")

    Solution4<-""


```

`@solution`
```{r}
Solution1 <- lm(opt_in~rainy,data=CreditCo)
Solution2 <- predict(Solution1,data=CreditCo)
CreditCo$prediction <- Solution2
Solution3 <- lm(balance_post ~ prediction,data=CreditCo)
Solution4 <- "increased"
```

`@sct`
```{r}
test_object("Solution1")
test_object("Solution2")
test_object("Solution3")
test_object("Solution4")
success_msg("Good work! The offer does seem to be effective, and our IV analysis tells us that the credit card offer caused customers to spend more money with their credit cards, which means CreditCo made more money on their interest payments.")
```

---

## Comparing the Estimates from Indirect Inference and 2SLS

```yaml
type: MultipleChoiceExercise
key: 458e4ed187
lang: r
xp: 50
skills: 1
```

The previous two coding exercises used two separate methods to estimate a causal effect using instrumental variables. The estimate from the indirect inference method yielded an effect of $16,319. The 2SLS method estimate was an identical $16,319.

In general, do you think that these two methods will always yield the same results?

`@possible_answers`
- Yes
- No

`@hint`


`@pre_exercise_code`
```{r}

```

`@sct`
```{r}
msg1 = "Try again"
msg2 = "Correct! The previous coding examples happened to yield identical results because of the specific situation. In general, the results should yield numbers that are 'statistically similar': that is, the confidence interval of one should contain the other."
test_mc(correct = 2, feedback_msgs = c(msg1,msg2))
```

---

## Solving Noncompliance in Experiments with Instrumental Variables

```yaml
type: VideoExercise
key: 25532d1a49
lang: r
xp: 50
skills: 1
video_link: //player.vimeo.com/video/219559688
```


---

## Overview of the Oregon Healthcare Experiment

```yaml
type: VideoExercise
key: 22d8b1d707
lang: r
xp: 50
skills: 1
video_link: //player.vimeo.com/video/304167565
```


---

## Using IV to Solve Noncompliance in the Oregon Heathcare Insurance Experiment

```yaml
type: VideoExercise
key: dc9b5ec411
lang: r
xp: 50
skills: 1
video_link: //player.vimeo.com/video/219559775
```


---

## Balancing Assumptions and Data When Using IV

```yaml
type: MultipleChoiceExercise
key: 23b590892d
lang: r
xp: 50
skills: 1
```

In previous videos we said that the relevance assumption is refutable, and in this video we saw that it passed our tests in the Oregon Healthcare experiment. We also said that the other two assumptions, exogeneity and exclusion, are nonrefutable because we would need a world of complete and perfect information to refute them. 

But two data analysts, Alex and Monique, are working together on an IV analysis, and they argue about whether more data can help them build a convincing argument for their instrument. Alex says, "there's no point to changing our model to include more datasets, because these last 2 assumptions are nonrefutable, so we can only use qualitative explanations to defend them." But Monique says that "we can replace our qualitative arguments with data-driven arguments if we add new datasets with the relevant data to our model."

Who is giving better advice for defending the 'nonrefutable' assumptions of IV?

`@possible_answers`
- Alex is correct: the assumptions are nonrefutable without perfect information, so more data won't help with them 
- Monique is correct: it's better to add more datasets to reduce the guesswork about the model

`@hint`


`@pre_exercise_code`
```{r}

```

`@sct`
```{r}
msg1 = "'Refutability' is defined with respect to a model, which includes a description of the precise set of data which is observable. For example, suppose I observe a dataset with the age of person in the United States. Then the claim that the average age is 26 is refutable—-I can simply compute the true average age and then see if it really is 26 or not. But the claim that the average annual income is $30,000 is not refutable. That’s just because my dataset doesn’t have income in it! If I observed data on income, then that claim would become refutable. Try again."
msg2 = "Correct. Without more or different kinds of data, typically the exogeneity and exclusion assumptions are not refutable, and Alex would be right. However, if you add more data or change the assumptions in different ways, it is sometimes possible to convert a nonrefutable assumption into a refutable assumption. But you should always be ready with qualitative arguments to strengthen your defense of these assumptions."
test_mc(correct = 2, feedback_msgs = c(msg1,msg2))
```

---

## Let's Code: Practice Using IV to Solve Noncompliance in the OHIE

```yaml
type: VideoExercise
key: 1ce6f872eb
lang: r
xp: 50
skills: 1
video_link: //player.vimeo.com/video/306239756
```


---

## Practice Using IV to Solve Noncompliance in the OHIE

```yaml
type: NormalExercise
key: 834d82578f
lang: r
xp: 100
skills: 1
```

In an earlier course, you analyzed data from the Oregon Healthcare Insurance Experiment (OHIE) where you assumed that everyone who was offered Medicaid coverage ended up getting covered. Now let us relax this assumption and instead assume that not everyone who was offered Medicaid coverage actually enrolled.

Instead, we will instrument for Medicaid takeup using assignment to the treatment group as an instrument.

`@instructions`
- 1) Explore the data on your own, using functions like head(), str(), and summary()
- 2) Compute the Intention-to-Treat effect of being offered insurance on having a positive depression screening
- 3) Estimate the 2SLS effect of taking up the offer on positive depression screening using two separate calls to `lm()`

`@hint`
- Remember to include additional demographic controls in your regression model

`@pre_exercise_code`
```{r}
set.seed(1)

#------------------------------------------
# Population characteristics for data generation
#------------------------------------------
n                          <- 20745
n_intvw                    <- 12229
frac_intvw                 <- (n_intvw-.5)/n
frac_treated               <- (6387-52)/n_intvw
frac_female                <- .5656
frac_19_34                 <- .3530
frac_35_49                 <- .3710
frac_50_64                 <- .2758
frac_white                 <- .688
frac_black                 <- .1033
frac_other                 <- .144
frac_hispa                 <- .180
frac_intv_english          <- .876
frac_ast_dx_pre_lottery    <- .193065
frac_dia_dx_pre_lottery    <- .071305
frac_hbp_dx_pre_lottery    <- .181944
frac_chl_dx_pre_lottery    <- .126666
frac_ami_dx_pre_lottery    <- .019789
frac_chf_dx_pre_lottery    <- .011121
frac_emp_dx_pre_lottery    <- .022078
frac_kid_dx_pre_lottery    <- .018562
frac_cancer_dx_pre_lottery <- .042767
frac_dep_dx_pre_lottery    <- .340665
frac_any_dx_pre_lottery    <- .159749
#------------------------------------------
# Initialize dataframe
#------------------------------------------
OHIE <- as.data.frame(matrix(0, ncol=23,nrow=n))

colnames(OHIE) <- c("id",
"intvw",
"lottery",
"gender_inp",
"age_19_34_inp",
"age_35_49_inp",
"age_50_64_inp",
"race_white_inp",
"race_black_inp",
"race_nwother_inp",
"hispanic_inp",
"itvw_english_inp",
"ast_dx_pre_lottery",
"dia_dx_pre_lottery",
"hbp_dx_pre_lottery",
"chl_dx_pre_lottery",
"ami_dx_pre_lottery",
"chf_dx_pre_lottery",
"emp_dx_pre_lottery",
"kid_dx_pre_lottery",
"cancer_dx_pre_lottery",
"dep_dx_pre_lottery",
"any_dx_pre_lottery")#Factors=factor(),

#------------------------------------------
# Simulate pre-lottery data
#------------------------------------------
OHIE$id                                                        <- seq(1,n,1)
OHIE$intvw                                                     <- as.integer(runif(n)<frac_intvw)
OHIE[OHIE$intvw==0,c("lottery","gender_inp","age_19_34_inp","age_35_49_inp","age_50_64_inp","race_white_inp","race_black_inp","race_nwother_inp","hispanic_inp","itvw_english_inp","ast_dx_pre_lottery","dia_dx_pre_lottery","hbp_dx_pre_lottery","chl_dx_pre_lottery","ami_dx_pre_lottery","chf_dx_pre_lottery","emp_dx_pre_lottery","kid_dx_pre_lottery","cancer_dx_pre_lottery","dep_dx_pre_lottery")] <- NA
OHIE$lottery[!is.na(OHIE$lottery)]                         <- as.integer(runif(n_intvw)<frac_treated)
OHIE$gender_inp[!is.na(OHIE$gender_inp)]                       <- as.integer(runif(n_intvw)<frac_female)
classdraw                                                      <- runif(n_intvw)
class                                                          <- 1*(classdraw<frac_19_34) + 2*(classdraw<(frac_19_34+frac_35_49) & classdraw>frac_19_34) + 3*(classdraw>(frac_19_34+frac_35_49))
OHIE$age_19_34_inp[!is.na(OHIE$age_19_34_inp)]                 <- as.integer(class==1)
OHIE$age_35_49_inp[!is.na(OHIE$age_35_49_inp)]                 <- as.integer(class==2)
OHIE$age_50_64_inp[!is.na(OHIE$age_50_64_inp)]                 <- as.integer(class==3)
OHIE$race_white_inp[!is.na(OHIE$race_white_inp)]               <- as.integer(runif(n_intvw)<frac_white)
OHIE$race_black_inp[!is.na(OHIE$race_black_inp)]               <- as.integer(runif(n_intvw)<frac_black)
OHIE$race_nwother_inp[!is.na(OHIE$race_nwother_inp)]           <- as.integer(runif(n_intvw)<frac_other)
OHIE$hispanic_inp[!is.na(OHIE$hispanic_inp)]                   <- as.integer(runif(n_intvw)<frac_hispa)
OHIE$itvw_english_inp[!is.na(OHIE$itvw_english_inp)]           <- as.integer(runif(n_intvw)<frac_hispa)
OHIE$ast_dx_pre_lottery[!is.na(OHIE$ast_dx_pre_lottery)]       <- as.integer(runif(n_intvw)<frac_ast_dx_pre_lottery)
OHIE$dia_dx_pre_lottery[!is.na(OHIE$dia_dx_pre_lottery)]       <- as.integer(runif(n_intvw)<frac_dia_dx_pre_lottery)
OHIE$hbp_dx_pre_lottery[!is.na(OHIE$hbp_dx_pre_lottery)]       <- as.integer(runif(n_intvw)<frac_hbp_dx_pre_lottery)
OHIE$chl_dx_pre_lottery[!is.na(OHIE$chl_dx_pre_lottery)]       <- as.integer(runif(n_intvw)<frac_chl_dx_pre_lottery)
OHIE$ami_dx_pre_lottery[!is.na(OHIE$ami_dx_pre_lottery)]       <- as.integer(runif(n_intvw)<frac_ami_dx_pre_lottery)
OHIE$chf_dx_pre_lottery[!is.na(OHIE$chf_dx_pre_lottery)]       <- as.integer(runif(n_intvw)<frac_chf_dx_pre_lottery)
OHIE$emp_dx_pre_lottery[!is.na(OHIE$emp_dx_pre_lottery)]       <- as.integer(runif(n_intvw)<frac_emp_dx_pre_lottery)
OHIE$kid_dx_pre_lottery[!is.na(OHIE$kid_dx_pre_lottery)]       <- as.integer(runif(n_intvw)<frac_kid_dx_pre_lottery)
OHIE$cancer_dx_pre_lottery[!is.na(OHIE$cancer_dx_pre_lottery)] <- as.integer(runif(n_intvw)<frac_cancer_dx_pre_lottery)
OHIE$dep_dx_pre_lottery[!is.na(OHIE$dep_dx_pre_lottery)]       <- as.integer(runif(n_intvw)<frac_dep_dx_pre_lottery)
OHIE$dep_dx_pre_lottery[!is.na(OHIE$dep_dx_pre_lottery)]       <- as.integer(runif(n_intvw)<frac_any_dx_pre_lottery)

#------------------------------------------
# Variable labels
#------------------------------------------
names(OHIE)[(length(OHIE)-10):length(OHIE)] <- c("asthma pre lottery","diabetes pre lottery","hypertension pre lottery","high cholesterol pre lottery","heart attack pre lottery","congestive heart failure pre lottery","emphysema/COPD pre lottery","kidney failure pre lottery","cancer pre lottery","depression pre lottery","any of diabetes, high bp, high chol, heart attack, or cong heart failure pre lottery")

#------------------------------------------
# Simulate first stage
#------------------------------------------
OHIE$insurance <- 0*(OHIE$lottery)
OHIE[OHIE$intvw==0,c("insurance")] <- NA
colnames(OHIE)[length(OHIE)] <- c("insurance")

draw <- runif(n_intvw)
xb   <- -2.82+1*(OHIE$lottery[!is.na(OHIE$lottery)]==1)+.75*(OHIE$gender_inp[!is.na(OHIE$lottery)]==1)+.5*(OHIE$age_19_34_inp[!is.na(OHIE$lottery)]==1)+1*(OHIE$age_35_49_inp[!is.na(OHIE$lottery)]==1)+2*(OHIE$age_50_64_inp[!is.na(OHIE$lottery)]==1)+1.3*(OHIE$race_white_inp[!is.na(OHIE$lottery)]==1)-.4*(OHIE$race_black_inp[!is.na(OHIE$lottery)]==1)-.2*(OHIE$race_nwother_inp[!is.na(OHIE$lottery)]==1)+.1*(OHIE$hispanic_inp[!is.na(OHIE$lottery)]==1)-.7*(OHIE$itvw_english_inp[!is.na(OHIE$lottery)]==1)
p    <- exp(xb)/(1+exp(xb))
OHIE$insurance[!is.na(OHIE$insurance)] <- as.integer(draw<p)

#------------------------------------------
# Simulate second stage
#------------------------------------------
OHIE<-cbind(OHIE,matrix(0,dim(OHIE)[1],19))
colnames(OHIE)[(length(OHIE)-18):length(OHIE)]             <- c("bp_sar_inp","bp_dar_inp","bp_hyper","hbp_dx_post_insurance","hbp_diure_med_inp","chl_inp","chl_h","hdl_inp","hdl_low","chl_dx_post_insurance","antihyperlip_med_inp","a1c_inp","a1c_dia","dia_dx_post_insurance","diabetes_med_inp","pos_depression_screen","dep_dx_post_insurance","antidep_med_inpbinary","cvd_risk_point")
OHIE[OHIE$intvw==0,c("bp_sar_inp","bp_dar_inp","bp_hyper","hbp_dx_post_insurance","hbp_diure_med_inp","chl_inp","chl_h","hdl_inp","hdl_low","chl_dx_post_insurance","antihyperlip_med_inp","a1c_inp","a1c_dia","dia_dx_post_insurance","diabetes_med_inp","pos_depression_screen","dep_dx_post_insurance","antidep_med_inpbinary","cvd_risk_point")] <- NA

# # Hypertension diagnosis (should be .056 + .0176)
# draw <- runif(n_intvw)
# xb   <- -2.82+.22*(OHIE$insurance[!is.na(OHIE$hbp_dx_post_insurance)]==1)
# p    <- exp(xb)/(1+exp(xb))
# OHIE$hbp_dx_post_insurance[!is.na(OHIE$hbp_dx_post_insurance)] <- as.integer(draw<p)
# print("Hypertension")
# print(mean(OHIE$hbp_dx_post_insurance[OHIE$insurance==0],na.rm=TRUE))
# print(mean(OHIE$hbp_dx_post_insurance[OHIE$insurance==1],na.rm=TRUE)-mean(OHIE$hbp_dx_post_insurance[OHIE$insurance==0],na.rm=TRUE))

# # Current use of hypertension medication (should be .139 + .0066)
# draw <- runif(n_intvw)
# xb   <- -1.825+.0716*(OHIE$insurance[!is.na(OHIE$hbp_diure_med_inp)]==1)
# p    <- exp(xb)/(1+exp(xb))
# OHIE$hbp_diure_med_inp[!is.na(OHIE$hbp_diure_med_inp)] <- as.integer(draw<p)
# print("Hypertension Meds")
# print(mean(OHIE$hbp_diure_med_inp[OHIE$insurance==0],na.rm=TRUE))
# print(mean(OHIE$hbp_diure_med_inp[OHIE$insurance==1],na.rm=TRUE)-mean(OHIE$hbp_diure_med_inp[OHIE$insurance==0],na.rm=TRUE))

# # Hypercholesterolemia diagnosis after insurance (should be .061 + .0239)
# draw <- runif(n_intvw)
# xb   <- -2.72+.371*(OHIE$insurance[!is.na(OHIE$chl_dx_post_insurance)]==1)
# p    <- exp(xb)/(1+exp(xb))
# OHIE$chl_dx_post_insurance[!is.na(OHIE$chl_dx_post_insurance)] <- as.integer(draw<p)
# print("Hypercholesterolemia")
# print(mean(OHIE$chl_dx_post_insurance[OHIE$insurance==0],na.rm=TRUE))
# print(mean(OHIE$chl_dx_post_insurance[OHIE$insurance==1],na.rm=TRUE)-mean(OHIE$chl_dx_post_insurance[OHIE$insurance==0],na.rm=TRUE))

# # Current use of high cholesterol medication (should be .085 + .0380)
# draw <- runif(n_intvw)
# xb   <- -2.36+.384*(OHIE$insurance[!is.na(OHIE$antihyperlip_med_inp)]==1)
# p    <- exp(xb)/(1+exp(xb))
# OHIE$antihyperlip_med_inp[!is.na(OHIE$antihyperlip_med_inp)] <- as.integer(draw<p)
# print("Hypercholesterolemia Meds")
# print(mean(OHIE$antihyperlip_med_inp[OHIE$insurance==0],na.rm=TRUE))
# print(mean(OHIE$antihyperlip_med_inp[OHIE$insurance==1],na.rm=TRUE)-mean(OHIE$antihyperlip_med_inp[OHIE$insurance==0],na.rm=TRUE))

# Positive depression screen (should be .3 - .0915)
draw <- runif(n_intvw)
xb   <- -.828-.55*(OHIE$insurance[!is.na(OHIE$pos_depression_screen)]==1)-.8*(OHIE$gender_inp[!is.na(OHIE$lottery)]==1)+.5*(OHIE$age_19_34_inp[!is.na(OHIE$lottery)]==1)+1*(OHIE$age_35_49_inp[!is.na(OHIE$lottery)]==1)+2*(OHIE$age_50_64_inp[!is.na(OHIE$lottery)]==1)+1.3*(OHIE$race_white_inp[!is.na(OHIE$lottery)]==1)-.4*(OHIE$race_black_inp[!is.na(OHIE$lottery)]==1)-.2*(OHIE$race_nwother_inp[!is.na(OHIE$lottery)]==1)+.1*(OHIE$hispanic_inp[!is.na(OHIE$lottery)]==1)-.7*(OHIE$itvw_english_inp[!is.na(OHIE$lottery)]==1)
p    <- exp(xb)/(1+exp(xb))
OHIE$pos_depression_screen[!is.na(OHIE$pos_depression_screen)] <- as.integer(draw<p)
# print("Positive depression screen")
# print(mean(OHIE$pos_depression_screen[OHIE$insurance==0],na.rm=TRUE))
# print(mean(OHIE$pos_depression_screen[OHIE$insurance==1],na.rm=TRUE)-mean(OHIE$pos_depression_screen[OHIE$insurance==0],na.rm=TRUE))

# # Depression diagnosis after insurance (should be .048 + .0381)
# draw <- runif(n_intvw)
# xb   <- -3.22+.571*(OHIE$insurance[!is.na(OHIE$dep_dx_post_insurance)]==1)
# p    <- exp(xb)/(1+exp(xb))
# OHIE$dep_dx_post_insurance[!is.na(OHIE$dep_dx_post_insurance)] <- as.integer(draw<p)
# print("Depression diagnosis after insurance")
# print(mean(OHIE$dep_dx_post_insurance[OHIE$insurance==0],na.rm=TRUE))
# print(mean(OHIE$dep_dx_post_insurance[OHIE$insurance==1],na.rm=TRUE)-mean(OHIE$dep_dx_post_insurance[OHIE$insurance==0],na.rm=TRUE))

# # Current use of depression meds (should be .168 + .0549)
# draw <- runif(n_intvw)
# xb   <- -1.825+.46*(OHIE$insurance[!is.na(OHIE$antidep_med_inpbinary)]==1)
# p    <- exp(xb)/(1+exp(xb))
# OHIE$antidep_med_inpbinary[!is.na(OHIE$antidep_med_inpbinary)] <- as.integer(draw<p)
# print("Current use of depression meds")
# print(mean(OHIE$antidep_med_inpbinary[OHIE$insurance==0],na.rm=TRUE))
# print(mean(OHIE$antidep_med_inpbinary[OHIE$insurance==1],na.rm=TRUE)-mean(OHIE$antidep_med_inpbinary[OHIE$insurance==0],na.rm=TRUE))

# # Diabetes diagnosis after insurance (should be .011 + .0383)
# draw <- runif(n_intvw)
# xb   <- -4.44+1.451*(OHIE$insurance[!is.na(OHIE$dia_dx_post_insurance)]==1)
# p    <- exp(xb)/(1+exp(xb))
# OHIE$dia_dx_post_insurance[!is.na(OHIE$dia_dx_post_insurance)] <- as.integer(draw<p)
# print("Diabetes diagnosis after insurance")
# print(mean(OHIE$dia_dx_post_insurance[OHIE$insurance==0],na.rm=TRUE))
# print(mean(OHIE$dia_dx_post_insurance[OHIE$insurance==1],na.rm=TRUE)-mean(OHIE$dia_dx_post_insurance[OHIE$insurance==0],na.rm=TRUE))

# # Continuous outcomes
# OHIE$bp_sar_inp[!is.na(OHIE$bp_sar_inp)]         <- rnorm(n_intvw,mean=119.3,sd=16.9)-.52*(OHIE$insurance[!is.na(OHIE$bp_sar_inp)]==1)
# OHIE$bp_dar_inp[!is.na(OHIE$bp_dar_inp)]         <- rnorm(n_intvw,mean= 76.0,sd=12.1)-.81*(OHIE$insurance[!is.na(OHIE$bp_dar_inp)]==1)
# OHIE$bp_hyper[!is.na(OHIE$bp_sar_inp)]           <- as.integer(OHIE$bp_sar_inp[!is.na(OHIE$bp_sar_inp)]>=140 & OHIE$bp_dar_inp[!is.na(OHIE$bp_sar_inp)]>=90)
# OHIE$chl_inp[!is.na(OHIE$chl_inp)]               <- rnorm(n_intvw,mean=204.1,sd=34.0)+2.2*(OHIE$insurance[!is.na(OHIE$chl_inp)]==1)
# OHIE$hdl_inp[!is.na(OHIE$hdl_inp)]               <- rnorm(n_intvw,mean= 47.6,sd=13.1)+2.2*(OHIE$insurance[!is.na(OHIE$hdl_inp)]==1)
# OHIE$chl_h[!is.na(OHIE$chl_inp)]                 <- as.integer(OHIE$chl_inp[!is.na(OHIE$chl_inp)]>=240)
# OHIE$hdl_low[!is.na(OHIE$hdl_inp)]               <- as.integer(OHIE$hdl_inp[!is.na(OHIE$hdl_inp)]< 40 )
# OHIE$a1c_inp[!is.na(OHIE$a1c_inp)]               <- rnorm(n_intvw,mean=  5.3,sd= 0.6)+0.1*(OHIE$insurance[!is.na(OHIE$hdl_inp)]==1)
# OHIE$a1c_dia[!is.na(OHIE$a1c_inp)]               <- as.integer(OHIE$a1c_inp[!is.na(OHIE$a1c_dia)]>=6.5)
# OHIE$cvd_risk_point[!is.na(OHIE$cvd_risk_point)] <- rnorm(n_intvw,mean=  8.2,sd= 7.5)-0.2*(OHIE$insurance[!is.na(OHIE$cvd_risk_point)]==1)

#------------------------------------------
# Pare down data
#------------------------------------------
OHIE <- OHIE[!is.na(OHIE$pos_depression_screen),c("id","intvw","lottery","gender_inp","age_19_34_inp","age_35_49_inp","age_50_64_inp","race_white_inp","race_black_inp","race_nwother_inp","hispanic_inp","itvw_english_inp","insurance","pos_depression_screen")]

# # Add code for ivreg2 function
# ivreg2 <- function(form,endog,iv,data,digits=3){
    # # library(MASS)
    # # model setup
    # r1 <- lm(form,data)
    # y <- r1$fitted.values+r1$resid
    # x <- model.matrix(r1)
    # aa <- rbind(endog == colnames(x),1:dim(x)[2])  
    # z <- cbind(x[,aa[2,aa[1,]==0]],data[,iv])  
    # colnames(z)[(dim(z)[2]-length(iv)+1):(dim(z)[2])] <- iv  
    # # iv coefficients and standard errors
    # z <- as.matrix(z)
    # pz <- z %*% (solve(crossprod(z))) %*% t(z)
    # biv <- solve(crossprod(x,pz) %*% x) %*% (crossprod(x,pz) %*% y)
    # sigiv <- crossprod((y - x %*% biv),(y - x %*% biv))/(length(y)-length(biv))
    # vbiv <- as.numeric(sigiv)*solve(crossprod(x,pz) %*% x)
    # res <- cbind(biv,sqrt(diag(vbiv)),biv/sqrt(diag(vbiv)),(1-pnorm(biv/sqrt(diag(vbiv))))*2)
    # res <- matrix(as.numeric(sprintf(paste("%.",paste(digits,"f",sep=""),sep=""),res)),nrow=dim(res)[1])
    # rownames(res) <- colnames(x)
    # colnames(res) <- c("Coef","S.E.","t-stat","p-val")
    # # First-stage F-test
    # y1 <- data[,endog]
    # z1 <- x[,aa[2,aa[1,]==0]]
    # bet1 <- solve(crossprod(z)) %*% crossprod(z,y1)
    # bet2 <- solve(crossprod(z1)) %*% crossprod(z1,y1)
    # rss1 <- sum((y1 - z %*% bet1)^2)
    # rss2 <- sum((y1 - z1 %*% bet2)^2)
    # p1 <- length(bet1)
    # p2 <- length(bet2)
    # n1 <- length(y)
    # fs <- abs((rss2-rss1)/(p2-p1))/(rss1/(n1-p1))
    # firststage <- c(fs)
    # firststage <- matrix(as.numeric(sprintf(paste("%.",paste(digits,"f",sep=""),sep=""),firststage)),ncol=length(firststage))
    # colnames(firststage) <- c("First Stage F-test")
    # # Hausman tests
    # bols <- solve(crossprod(x)) %*% crossprod(x,y) 
    # sigols <- crossprod((y - x %*% bols),(y - x %*% bols))/(length(y)-length(bols))
    # vbols <- as.numeric(sigols)*solve(crossprod(x))
    # sigml <- crossprod((y - x %*% bols),(y - x %*% bols))/(length(y))
    # x1 <- x[,!(colnames(x) %in% "(Intercept)")]
    # z1 <- z[,!(colnames(z) %in% "(Intercept)")]
    # pz1 <- z1 %*% (solve(crossprod(z1))) %*% t(z1)
    # biv1 <- biv[!(rownames(biv) %in% "(Intercept)"),]
    # bols1 <- bols[!(rownames(bols) %in% "(Intercept)"),]
    # # Durbin-Wu-Hausman chi-sq test:
    # # haus <- t(biv1-bols1) %*% ginv(as.numeric(sigml)*(solve(crossprod(x1,pz1) %*% x1)-solve(crossprod(x1)))) %*% (biv1-bols1)
    # # hpvl <- 1-pchisq(haus,df=1)
    # # Wu-Hausman F test
    # resids <- NULL
    # resids <- cbind(resids,y1 - z %*% solve(crossprod(z)) %*% crossprod(z,y1))
    # x2 <- cbind(x,resids)
    # bet1 <- solve(crossprod(x2)) %*% crossprod(x2,y)
    # bet2 <- solve(crossprod(x)) %*% crossprod(x,y)
    # rss1 <- sum((y - x2 %*% bet1)^2)
    # rss2 <- sum((y - x %*% bet2)^2)
    # p1 <- length(bet1)
    # p2 <- length(bet2)
    # n1 <- length(y)
    # fs <- abs((rss2-rss1)/(p2-p1))/(rss1/(n1-p1))
    # fpval <- 1-pf(fs, p1-p2, n1-p1)
    # #hawu <- c(haus,hpvl,fs,fpval)
    # hawu <- c(fs,fpval)
    # hawu <- matrix(as.numeric(sprintf(paste("%.",paste(digits,"f",sep=""),sep=""),hawu)),ncol=length(hawu))
    # #colnames(hawu) <- c("Durbin-Wu-Hausman chi-sq test","p-val","Wu-Hausman F-test","p-val")
    # colnames(hawu) <- c("Wu-Hausman F-test","p-val")  
    # # Sargan Over-id test
    # ivres <- y - (x %*% biv)
    # oid <- solve(crossprod(z)) %*% crossprod(z,ivres)
    # sstot <- sum((ivres-mean(ivres))^2)
    # sserr <- sum((ivres - (z %*% oid))^2)
    # rsq <- 1-(sserr/sstot)
    # sargan <- length(ivres)*rsq
    # spval <- 1-pchisq(sargan,df=length(iv)-1)
    # overid <- c(sargan,spval)
    # overid <- matrix(as.numeric(sprintf(paste("%.",paste(digits,"f",sep=""),sep=""),overid)),ncol=length(overid))
    # colnames(overid) <- c("Sargan test of over-identifying restrictions","p-val")
    # if(length(iv)-1==0){
      # overid <- t(matrix(c("No test performed. Model is just identified")))
      # colnames(overid) <- c("Sargan test of over-identifying restrictions")
    # }
    # full <- list(results=res, weakidtest=firststage, endogeneity=hawu, overid=overid)
    # return(full)
# }
```

`@sample_code`
```{r}
# The dataset `OHIE` and the function `ivreg2` are both available in your workspace. Here is a data dictionary to help you make sense of the variable names and what they are representing:

*Data Dictionary*

#  `id` - patient ID
#  `intvw` - was patient interviewed?
#  `lottery` - did patient win in the Medicaid lottery?
#  `gender_inp` - gender, where 1 is female*
#  `age_19_34_inp` - 1 if age is between 19 and 34*
#  `age_35_49_inp` - 1 if age is between 35 and 49*
#  `age_50_64_inp` - 1 if age is between 50 and 64*
#  `race_white_inp` -  1 if race is white*
#  `race_black_inp` - 1 if race is black*
#  `race_nwother_inp` - 1 if race is nonwhite and nonblack*
#  `hispanic_inp` - 1 if hispanic*
#  `itvw_english_inp` - 1 if interview was held in english*
#  `insurance` - did patient have health insurance at all?
#  `pos_depression_screen` - 1 if patient tested positive for depression
#  *note: `_inp` variable suffix - recorded during in-person interview

# 1) On your own, explore the dataframe and get familiar with the ranges of values and data types it contains. We suggest using commands like head(), str(), and summary() for different variables to get a sense of what the data look like. 





# 2) Let's see what answer we get if we treat the lottery as an experiment where we consider getting the option for Medicaid as our treatment, and see if it shows any effect on our outcome of interest, the ratio of peopel with positive depression screens. Using a linear regression, compute the Intention-to-Treat effect of being assigned to the treatment group ("lottery" variable) on positive depression screening, while also controlling for gender, age, ethnicity, and interview language. Exclude `age_19_34_inp` and `race_white_inp` from the set of right hand side dummy variables.

    Solution1<-summary(lm( ~ ,data=OHIE))

# Good. If you take a look at Solution1, you'll see that having the option of getting insurance through the lotter seems to have a small downward pressure on our positive depression screen variable, which is good! But is this really a causal effect? Let's try again with IV analysis and see what we get.

# 3) We'll use the 2-Stage Least Squares method to calculate our causal effect through our instrumental variable, `lottery`. The first stage is to find the correlation between the treatment and the instrument, which we will store in the variable `prediction`, and the second stage is to find the correlation between our outcome and our adjusted treatment variable `prediction`, holding the other variables constant. This will give us the causal effect of health insurance on our outcome of interest, the ratio of people with positive depression screen scores.

# Compute the the 2SLS effect of insurance takeup on depression by using `lottery` as an instrument for the endogenous takeup indicator "insurance".

    OHIE$prediction<-predict(lm( ~ ,data=OHIE))
    Solution2<-summary(lm( ~ prediction+... ,data=OHIE))

```

`@solution`
```{r}
Solution1<-summary(lm(pos_depression_screen~lottery+gender_inp+age_35_49_inp+age_50_64_inp+race_black_inp+race_nwother_inp+hispanic_inp+itvw_english_inp,data=OHIE))
OHIE$prediction<-predict(lm(insurance~lottery+gender_inp+age_35_49_inp+age_50_64_inp+race_black_inp+race_nwother_inp+hispanic_inp+itvw_english_inp,data=OHIE),data=OHIE)
Solution2<-summary(lm(pos_depression_screen~prediction+gender_inp+age_35_49_inp+age_50_64_inp+race_black_inp+race_nwother_inp+hispanic_inp+itvw_english_inp,data=OHIE),data=OHIE)
```

`@sct`
```{r}
test_object("Solution1")
test_object("Solution2")
success_msg("Good work! This estimate for the causal effect of insurance on our outcome variable, positive depression screens, is about 5 times larger than we saw in the Intention-to-Treat analysis. It looks like health insurance does improve health outcomes, at least for this variable.")
```
