--- 
title       : "RDD as Causality: Introduction to Regression Discontinuity Design"
description : "This chapter will introduce you to regression discontinuity design (RDD)"
 
# Notes:
#--------------
# Deciding what constitutes "close" to the cutoff
#  - want to increase BW to get more data
#  - want to decrease BW to remove confounding bias
# Nonparametric (local linear) vs. parametric (separate regressions)
# Balance checks (McCrary test, etc.)
# RD as DID
# Sharp vs. Fuzzy: how do you know it's not fuzzy? outcome variable is either 0 or 1


--- type:VideoExercise lang:r aspect_ratio:62.5 xp:50 skills:1 key:efc08cc323
## Regression Discontinuity: Looking at People on the Edge
*** =video_link
//player.vimeo.com/video/218501284


--- type:MultipleChoiceExercise lang:r xp:50 skills:1 key:f401e635a5
## Key variables in an RD design
Which of the following variables is *not* a key (required) variable in a regression discontinuity design?

*** =instructions
- Outcome variable
- Control variable
- Treatment variable
- Running variable
*** =sct
```{r}
msg1 = "The outcome variable is the variable of interest. Try again"
msg2 = "Correct! While most RD's use at least one control variable, doing so is not strictly necessary for RD analysis. (i.e. it is possible, but not common, to run an RD without any control variables.)"
msg3 = "The treatment variable is necessary in order to estimate any kind of causal effect. Try again"
msg4 = "The running variable is how the researcher divides the sample into treatment and control groups. Try again"
test_mc(correct = 2, feedback_msgs = c(msg1,msg2,msg3,msg4))
```

--- type:VideoExercise lang:r aspect_ratio:62.5 xp:50 skills:1 key:79965f29b5
## Regression Discontinuity: More Analysis of Thistlethwaite and Campbell
*** =video_link
//player.vimeo.com/video/218502333


--- type:MultipleChoiceExercise lang:r xp:50 skills:1 key:7977c48f95
## Practice identifying key variables in an RD design
A pair of researchers named Jonah Berger and Devin Pope recently published a study that found that losing can lead to winning. As part of the study, the researchers documented this phenomenon in National Basketball Association (NBA) games. The researchers studied thousands of NBA game outcomes and found that teams who were just behind at halftime were more likely to win the game, after controling for other important factors (like the winning percentage of each team). 

In this study, which variable is the outcome variable, which is the running variable, and which is the treatment variable?

*** =instructions
- The score at halftime is the outcome variable, winning the game is the running variable, and the home team losing at halftime is the treatment variable.
- Winning the game is the outcome variable, the home team losing at halftime is the running variable, and the score at halftime is the treatment variable.
- The score at halftime is the outcome variable, the home team losing at halftime is the running variable, and winning the game is the treatment variable.
- Winning the game is the outcome variable, the score at halftime is the running variable, and the home team losing at halftime is the treatment variable.
*** =sct
```{r}
msg1 = "Try again"
msg2 = "Try again"
msg3 = "Try again"
msg4 = "Correct!"
test_mc(correct = 4, feedback_msgs = c(msg1,msg2,msg3,msg4))
```


--- type:VideoExercise lang:r aspect_ratio:62.5 xp:50 skills:1 key:1a5dfd94ba
## How to Compute Causal Effects in a Regression Discontinuity Analysis
*** =video_link
//player.vimeo.com/video/218502963



--- type:MultipleChoiceExercise lang:r xp:50 skills:1 key:f1b89ac666
## Visualizing RD designs
Most analyses using the RDD approach graphically show the causal effect at the discontinuity. The graph on the right hand part of your screen displays this for the NBA game data mentioned in a previous exercise of this chapter.

Each dot on the plot corresponds to the average margin of victory for the home team taken from a large number of games that had the same halftime margin for the home team. 

As you can see, there is a slight break in the two lines right at a margin of zero, implying that teams who are slightly behind at halftime are slightly more likely (than otherwise expected) to end the game with a positive margin (i.e. win the game).

Why do you think the lines in this graph are sloping upward?

*** =instructions
- Because the RDD isn't controlling for team quality in an appropriate way
- Because we should expect teams that are farther ahead at halftime to have a higher chance of winning, regardless of team quality or any other attribute
- Because there is an inherent home court advantage in the NBA

*** =pre_exercise_code
```{r}
set.seed(1)
n <- 18060

# Initialize dataframe
NBA <- as.data.frame(matrix(0, ncol=10,nrow=n))
colnames(NBA) <- c("id","season","home.team","away.team","home.team.qual","away.team.qual","home.team.halftime.score","away.team.halftime.score","home.team.final.margin","home.team.win")

# Simulate baseline data
NBA$id         <- seq(1,n,1)
NBA$season     <- ifelse(NBA$id %in% seq( 0*1204+1, 1*1204,1),1994,
                  ifelse(NBA$id %in% seq( 1*1204+1, 2*1204,1),1995,
                  ifelse(NBA$id %in% seq( 2*1204+1, 3*1204,1),1996,
                  ifelse(NBA$id %in% seq( 3*1204+1, 4*1204,1),1997,
                  ifelse(NBA$id %in% seq( 4*1204+1, 5*1204,1),1998,
                  ifelse(NBA$id %in% seq( 5*1204+1, 6*1204,1),1999,
                  ifelse(NBA$id %in% seq( 6*1204+1, 7*1204,1),2000,
                  ifelse(NBA$id %in% seq( 7*1204+1, 8*1204,1),2001,
                  ifelse(NBA$id %in% seq( 8*1204+1, 9*1204,1),2002,
                  ifelse(NBA$id %in% seq( 9*1204+1,10*1204,1),2003,
                  ifelse(NBA$id %in% seq(10*1204+1,11*1204,1),2004,
                  ifelse(NBA$id %in% seq(11*1204+1,12*1204,1),2005,
                  ifelse(NBA$id %in% seq(12*1204+1,13*1204,1),2006,
                  ifelse(NBA$id %in% seq(13*1204+1,14*1204,1),2007,
                  ifelse(NBA$id %in% seq(14*1204+1,15*1204,1),2008,0)))))))))))))))
teamqual <- cbind(c("BOS","NJN","NYK","PHL","GSW","LAC","LAL","PHX","SAC","CHI","CLE","DET","IND","MIL","DAL","HOU","MEM","SAS","ATL","CHA","MIA","ORL","WSH","DEN","MIN","SEA","POR","UTA"),
                  matrix(rnorm(28*15,mean=0,sd=1), ncol=15))
NBA$home.team      <- sample(teamqual[,1],n,replace=TRUE)
NBA$away.team      <- sample(teamqual[,1],n,replace=TRUE)
print(paste0('Number of teams playing themselves: ',sum(NBA$home.team==NBA$away.team)/n*100,'%'))
for (i in 1:n) {
    if (NBA$home.team[i]==NBA$away.team[i]) {
        NBA$away.team[i] <- sample(setdiff(teamqual[,1],NBA$home.team[i]),1)
    }
}
print(paste0('Number of teams playing themselves: ',sum(NBA$home.team==NBA$away.team)/n*100,'%'))

# generate team qualities to explain outcomes for each game
for (i in 1:n) {
    NBA$away.team.qual[i] <- as.numeric(teamqual[teamqual[,1]==NBA$away.team[i],NBA$season[i]-1992])
    NBA$home.team.qual[i] <- as.numeric(teamqual[teamqual[,1]==NBA$home.team[i],NBA$season[i]-1992])
}

NBA$home.team <- as.factor(NBA$home.team)
NBA$away.team <- as.factor(NBA$away.team)

# Generate halftime scores
NBA$home.team.halftime.score  <- round(NBA$home.team.qual+rnorm(n, mean=48, sd=4), digits=0)
NBA$away.team.halftime.score  <- round(NBA$away.team.qual+rnorm(n, mean=47, sd=6), digits=0)
NBA$home.team.halftime.margin <- NBA$home.team.halftime.score - NBA$away.team.halftime.score

# Remove games that are tied at halftime
print(dim(NBA))
NBA <- NBA[NBA$home.team.halftime.margin!=0,]
print(dim(NBA))
n1 <- dim(NBA)[1]

# Generate final score
NBA$home.team.winning.at.half <- NBA$home.team.halftime.margin>0
NBA$home.team.final.margin    <- round(2 - 5*NBA$home.team.winning.at.half + 2*NBA$home.team.halftime.margin -.05*NBA$home.team.halftime.margin^2 + NBA$home.team.qual - NBA$away.team.qual + rnorm(n1, mean=0, sd=10), digits=0)
NBA$home.team.win             <- NBA$home.team.final.margin>0

# Clean up data
NBA$season <- as.factor(NBA$season)
NBA <- NBA[,c("id","season","home.team","away.team","home.team.qual","away.team.qual","home.team.halftime.margin","home.team.final.margin","home.team.winning.at.half","home.team.win")]


# Create model
    fit<-lm(home.team.final.margin ~ home.team.winning.at.half + 
         home.team.halftime.margin +
         home.team.qual + away.team.qual, data = NBA)


# Create bins
    bins<-seq(-10,10,(20/15))
    
# Create predicted values
    # Create df with all model values except margins set at 0 or F
        NBA2<-NBA[1:16,]
        NBA2$home.team.winning.at.half<-rep(c(F,T),each=8)
        NBA2$home.team.qual<-0
        NBA2$away.team.qual<-0
        NBA2$home.team.halftime.margin<-bins
    # Predict values
        predicts<- predict(fit, NBA2)

#Turn margins and predicts into df
    df<-data.frame(predicts=predicts, margins=NBA2$home.team.halftime.margin)
        
#create locations of line segments
    mod1<-lm(predicts~margins,data=df[1:8,])
    mod2<-lm(predicts~margins,data=df[9:16,])
    
    y1.1<-mod1$coefficients[1]+mod1$coefficients[2]*-10
    y1.2<-mod1$coefficients[1]+mod1$coefficients[2]*0
    y2.1<-mod2$coefficients[1]+mod2$coefficients[2]*0
    y2.2<-mod2$coefficients[1]+mod2$coefficients[2]*10
  
#plot
    library(ggplot2)
    ggplot(df, aes(x = margins, y = predicts )) + 
      geom_vline(aes(xintercept = 0), color = 'grey', size = 1, linetype = 'dashed') +  # verticle line at discontinuity
      geom_point(size=3) + # adjust point size to your liking
      geom_segment(aes(x = df$margins[1], xend = 0, y = y1.1, yend = y1.2), size = 1)+
      geom_segment(aes(x = 0, xend = df$margins[16], y = y2.1, yend = y2.2), size = 1)

```

*** =sample_code
```{r}
    ggplot(df, aes(x = margins, y = predicts )) + 
      geom_vline(aes(xintercept = 0), color = 'grey', size = 1, linetype = 'dashed') +  # verticle line at discontinuity
      geom_point(size=3) + # adjust point size to your liking
      geom_segment(aes(x = df$margins[1], xend = 0, y = y1.1, yend = y1.2), size = 1)+
      geom_segment(aes(x = 0, xend = df$margins[16], y = y2.1, yend = y2.2), size = 1)+
      labs(x = "Margin of victory")+
      labs(y = "Probability of winning")
```
*** =sct
```{r}
msg1 = "Almost, try again."
msg2 = "Good job! This is an example of why conclusions from RDD analyses are only valid at the boundary of the discontinuity! If we get too far away, we will be picking up effects that will confound our causal inference."
msg3 = "Almost, try again."
test_mc(correct = 2, feedback_msgs = c(msg1,msg2,msg3))
```


--- type:NormalExercise lang:r aspect_ratio:62.5 xp:50 skills:1 key:8dfb92fd8e
## Practice Computing Regression Discontinuity Effects
Let's continue with the NBA research example. On your workspace is a simulated data set, called `NBA`, that closely resembles the data analyzed by Berger and Pope. The data frame `NBA` contains game characteristics for over 18,000 NBA games between 1994 and 2009.

The outcome variable we are interested in is called `home.team.final.margin`, which is the final margin of victory (or loss) for the home team. The running variable in this RDD is called `home.team.halftime.margin`, or the margin of victory (or loss) for the home team at halftime. We define treatment as the home team being ahead at halftime, i.e. `home.team.halftime.margin > 0`. (You can also define it as the home team being behind at halftime and you will get the same results, just opposite sign.)

In this exercise, you will compute the treatment effect of being ahead at halftime on the final margin of victory. You will use regression methods as well as nonparametric methods to assess how robust the effect is.

*** =instructions
- Use OLS regression to estimate the treatment effect of being behind at halftime on the final margin of victory under two different parametric scenarios
- Estimate the treatment effect using non-parametric methods

*** =pre_exercise_code
```{r}
# require(rdd)

set.seed(1)
n <- 18060

# Initialize dataframe
NBA <- as.data.frame(matrix(0, ncol=10,nrow=n))
colnames(NBA) <- c("id","season","home.team","away.team","home.team.qual","away.team.qual","home.team.halftime.score","away.team.halftime.score","home.team.final.margin","home.team.win")

# Simulate baseline data
NBA$id         <- seq(1,n,1)
NBA$season     <- ifelse(NBA$id %in% seq( 0*1204+1, 1*1204,1),1994,
                  ifelse(NBA$id %in% seq( 1*1204+1, 2*1204,1),1995,
                  ifelse(NBA$id %in% seq( 2*1204+1, 3*1204,1),1996,
                  ifelse(NBA$id %in% seq( 3*1204+1, 4*1204,1),1997,
                  ifelse(NBA$id %in% seq( 4*1204+1, 5*1204,1),1998,
                  ifelse(NBA$id %in% seq( 5*1204+1, 6*1204,1),1999,
                  ifelse(NBA$id %in% seq( 6*1204+1, 7*1204,1),2000,
                  ifelse(NBA$id %in% seq( 7*1204+1, 8*1204,1),2001,
                  ifelse(NBA$id %in% seq( 8*1204+1, 9*1204,1),2002,
                  ifelse(NBA$id %in% seq( 9*1204+1,10*1204,1),2003,
                  ifelse(NBA$id %in% seq(10*1204+1,11*1204,1),2004,
                  ifelse(NBA$id %in% seq(11*1204+1,12*1204,1),2005,
                  ifelse(NBA$id %in% seq(12*1204+1,13*1204,1),2006,
                  ifelse(NBA$id %in% seq(13*1204+1,14*1204,1),2007,
                  ifelse(NBA$id %in% seq(14*1204+1,15*1204,1),2008,0)))))))))))))))
teamqual <- cbind(c("BOS","NJN","NYK","PHL","GSW","LAC","LAL","PHX","SAC","CHI","CLE","DET","IND","MIL","DAL","HOU","MEM","SAS","ATL","CHA","MIA","ORL","WSH","DEN","MIN","SEA","POR","UTA"),
                  matrix(rnorm(28*15,mean=0,sd=1), ncol=15))
NBA$home.team      <- sample(teamqual[,1],n,replace=TRUE)
NBA$away.team      <- sample(teamqual[,1],n,replace=TRUE)
print(paste0('Number of teams playing themselves: ',sum(NBA$home.team==NBA$away.team)/n*100,'%'))
for (i in 1:n) {
    if (NBA$home.team[i]==NBA$away.team[i]) {
        NBA$away.team[i] <- sample(setdiff(teamqual[,1],NBA$home.team[i]),1)
    }
}
print(paste0('Number of teams playing themselves: ',sum(NBA$home.team==NBA$away.team)/n*100,'%'))

# generate team qualities to explain outcomes for each game
for (i in 1:n) {
    NBA$away.team.qual[i] <- as.numeric(teamqual[teamqual[,1]==NBA$away.team[i],NBA$season[i]-1992])
    NBA$home.team.qual[i] <- as.numeric(teamqual[teamqual[,1]==NBA$home.team[i],NBA$season[i]-1992])
}

NBA$home.team <- as.factor(NBA$home.team)
NBA$away.team <- as.factor(NBA$away.team)

# Generate halftime scores
NBA$home.team.halftime.score  <- round(NBA$home.team.qual+rnorm(n, mean=48, sd=4), digits=0)
NBA$away.team.halftime.score  <- round(NBA$away.team.qual+rnorm(n, mean=47, sd=6), digits=0)
NBA$home.team.halftime.margin <- NBA$home.team.halftime.score - NBA$away.team.halftime.score

# Remove games that are tied at halftime
print(dim(NBA))
NBA <- NBA[NBA$home.team.halftime.margin!=0,]
print(dim(NBA))
n1 <- dim(NBA)[1]

# Generate final score
NBA$home.team.winning.at.half <- NBA$home.team.halftime.margin>0
NBA$home.team.final.margin    <- round(2 - 5*NBA$home.team.winning.at.half + 2*NBA$home.team.halftime.margin -.05*NBA$home.team.halftime.margin^2 + NBA$home.team.qual - NBA$away.team.qual + rnorm(n1, mean=0, sd=10), digits=0)
NBA$home.team.win             <- NBA$home.team.final.margin>0

# Clean up data
NBA$season <- as.factor(NBA$season)
NBA <- NBA[,c("id","season","home.team","away.team","home.team.qual","away.team.qual","home.team.halftime.margin","home.team.final.margin","home.team.winning.at.half","home.team.win")]

Formula <- function(object) {

  stopifnot(inherits(object, "formula"))

  object_split <- split_formula(object)

  structure(object, lhs = object_split$lhs, rhs = object_split$rhs,
    class = c("Formula", "formula"))
}

as.Formula <- function(x, ...) UseMethod("as.Formula")

as.Formula.default <- function(x, ..., env = parent.frame()) Formula(as.formula(x, env = env))

as.Formula.Formula <- function(x, ...) x

as.Formula.formula <- function(x, ..., env) {

  ## preserve original environment
  if(missing(env)) env <- environment(x)

  ## combine all arguments to formula list
  x <- c(list(x), list(...))
  x <- lapply(x, as.formula)
  
  ## split all 
  x_split <- lapply(x, split_formula)
  x_lhs <- do.call("c", lapply(x_split, "[[", "lhs"))
  x_rhs <- do.call("c", lapply(x_split, "[[", "rhs"))

  ## recombine
  x_all <- paste_formula(x_lhs, x_rhs)
  
  ## create formula
  ## (we have everything to do this by hand, but for encapsulating code
  ## call Formula() again...which splits again)
  rval <- Formula(x_all)

  ## re-attach original environment
  environment(rval) <- env
  return(rval)
}

is.Formula <- function(object) inherits(object, "Formula")

formula.Formula <- function(x, lhs = NULL, rhs = NULL, collapse = FALSE,
  update = FALSE, drop = TRUE, ...)
{
  ## available parts
  lpart <- 1L:length(attr(x, "lhs"))
  rpart <- 1L:length(attr(x, "rhs"))

  ## default: keep all parts
  lhs <- if(is.null(lhs)) lpart else lpart[lhs]
  rhs <- if(is.null(rhs)) rpart else rpart[rhs]
  if(any(is.na(lhs))) {
    lhs <- as.vector(na.omit(lhs))
    if(length(lhs) < 1L) lhs <- 0L
    warning("subscript out of bounds, not all 'lhs' available")
  }
  if(any(is.na(rhs))) {
    rhs <- as.vector(na.omit(rhs))
    if(length(rhs) < 1L) rhs <- 0L
    warning("subscript out of bounds, not all 'rhs' available")
  }  

  ## collapse: keep parts separated by "|" or collapse with "+"
  collapse <- rep(as.logical(collapse), length.out = 2)

  rval <- paste_formula(attr(x, "lhs")[lhs], attr(x, "rhs")[rhs],
    lsep = ifelse(collapse[1L], "+", "|"),
    rsep = ifelse(collapse[2L], "+", "|"))

  ## omit potentially redundant terms
  if(all(collapse) & update) rval <- update(rval, if(length(rval) > 2) . ~ . else ~ .)

  ## reconvert to Formula if desired
  if(!drop) rval <- Formula(rval)

  ## re-attach original environment
  environment(rval) <- environment(x)

  return(rval)
}

terms.Formula <- function(x, ..., lhs = NULL, rhs = NULL, dot = "separate")
{
  ## simplify to standard formula
  form <- simplify_to_formula(x, lhs = lhs, rhs = rhs)

  ## if necessary try to expand/update/simplify formula parts with dot
  if(has_dot(form)) {
    x_orig <- x
    dot <- match.arg(dot, c("separate", "sequential"))

    ## lhs and rhs calls
    ll <- formula(x, rhs = 0L, collapse = TRUE)[[2L]]
    rr <- attr(x, "rhs")

    ## update and simplify again
    for(i in seq_along(rr)) {
      if(dot == "sequential" && i > 1L) ll <- c_formula(ll, rr[[i - 1L]], sep = "+")
      fi <- paste_formula(ll, rr[[i]]) #probably better than:# paste_formula(NULL, c_formula(rr[[i]], ll, sep = "-"))
      rr[[i]] <- update(formula(terms(fi, ...)), . ~ .)[[3L]]
    }
    attr(x, "rhs") <- rr
    form <- simplify_to_formula(x, lhs = lhs, rhs = rhs)

    ## call traditional terms()
    mt <- terms(form, ...)

    ## store updating for future reference (e.g., in model.part)
    attr(mt, "Formula_with_dot") <- x_orig
    attr(mt, "Formula_without_dot") <- x
    attr(mt, "dot") <- dot
  } else {
    ## call traditional terms()
    mt <- terms(form, ...)
  }
  
  return(mt)
}

model.frame.Formula <- function(formula, data = NULL, ..., lhs = NULL, rhs = NULL, dot = "separate")
{
  model.frame(terms(formula, lhs = lhs, rhs = rhs, data = data, dot = dot), data = data, ...)
}

model.matrix.Formula <- function(object, data = environment(object), ..., lhs = NULL, rhs = 1, dot = "separate")
{
  form <- formula(object, lhs = lhs, rhs = rhs, collapse = c(FALSE, TRUE))
  mt <- delete.response(terms(form, data = data, dot = dot))
  model.matrix(mt, data = data, ...)
}

## as model.response() is not generic, we do this:
model.part <- function(object, ...)
  UseMethod("model.part")

model.part.formula <- function(formula, data, ..., drop = FALSE) {
  formula <- Formula(formula)
  NextMethod()
}

model.part.Formula <- function(object, data, lhs = 0, rhs = 0, drop = FALSE, terms = FALSE, dot = NULL, ...) {

  ## *hs = NULL: keep all parts
  if(is.null(lhs)) lhs <- 1L:length(attr(object, "lhs"))
  if(is.null(rhs)) rhs <- 1L:length(attr(object, "rhs"))

  if(isTRUE(all.equal(as.numeric(lhs), rep(0, length(lhs)))) &
     isTRUE(all.equal(as.numeric(rhs), rep(0, length(rhs)))))
    stop("Either some 'lhs' or 'rhs' has to be selected.")

  if(is.null(dot)) {
    if(is.null(attr(attr(data, "terms"), "dot"))) {
      dot <- "separate"
    } else {
      dot <- attr(attr(data, "terms"), "dot")
    }
  } else {
    dot <- match.arg(dot, c("separate", "sequential"))
  }

  ##
  if(has_dot(object) &&
     !is.null(attr(data, "terms")) &&
     all(c("Formula_with_dot", "Formula_without_dot", "dot") %in% names(attributes(attr(data, "terms")))) &&
     dot == attr(attr(data, "terms"), "dot") &&
     simplify_to_formula(object, lhs = lhs, rhs = rhs) == simplify_to_formula(attr(attr(data, "terms"), "Formula_with_dot"), lhs = lhs, rhs = rhs)
  ) {
    object <- attr(attr(data, "terms"), "Formula_without_dot")
  }

  ## construct auxiliary terms object
  mt <- terms(object, lhs = lhs, rhs = rhs, dot = dot, data = data)

  ## subset model frame
  ix <- attr(mt, "variables")[-1L]
  if(is.null(ix)) {
    ix <- 0
  } else {
    ix <- sapply(ix, deparse)
    if(!all(ix %in% names(data))) stop(
      paste("'data' does not seem to be an appropriate 'model.frame':",
      paste(paste("'", ix[!(ix %in% names(data))], "'", sep = ""), collapse = ", "),
      "not found")
    )
  }
  rval <- data[, ix, drop = drop]
  if(!is.data.frame(rval)) names(rval) <- rownames(data)
  if(terms) attr(rval, "terms") <- mt
  return(rval)
}

update.Formula <- function(object, new,...) {

  new <- Formula(new)
  
  ## extract all building blocks
  o_lhs <- attr(object, "lhs")
  o_rhs <- attr(object, "rhs")
  n_lhs <- attr(new, "lhs")
  n_rhs <- attr(new, "rhs")
  lhs <- rep(list(NULL), length = max(length(o_lhs), length(n_lhs)))
  rhs <- rep(list(NULL), length = max(length(o_rhs), length(n_rhs)))

  ## convenience function for updating components
  update_components <- function(x, y) {
    xf <- yf <- ~ .
    xf[[2L]] <- x
    yf[[2L]] <- y
    update(xf, yf)[[2L]]
  }
    
  if(length(lhs) > 0L) for(i in 1L:length(lhs)) {
    lhs[[i]] <- if(length(o_lhs) < i) n_lhs[[i]]
      else if(length(n_lhs) < i) o_lhs[[i]]
      else update_components(o_lhs[[i]], n_lhs[[i]])
  }

  if(length(rhs) > 0L) for(i in 1L:length(rhs)) {
    rhs[[i]] <- if(length(o_rhs) < i) n_rhs[[i]]
      else if(length(n_rhs) < i) o_rhs[[i]]
      else update_components(o_rhs[[i]], n_rhs[[i]])
  }

  ## recombine
  rval <- paste_formula(lhs, rhs)
  
  ## create formula
  ## (we have everything to do this by hand, but for encapsulating code
  ## call Formula() again...which splits again)
  rval <- Formula(rval)  
  
  ## preserve original environment
  environment(rval) <- environment(object)
  
  return(rval)
}

length.Formula <- function(x) {
  ## NOTE: return length of both sides, not only rhs
  c(length(attr(x, "lhs")), length(attr(x, "rhs")))
}

print.Formula <- function(x, ...) {
  ## we could avoid calling formula() by computing on the internal
  ## structure attr(x, "rhs") <- attr(x, "lhs") <- NULL
  ## but this is probably cleaner...
  print(formula(x))
  invisible(x)
}

all.equal.Formula <- function(target, current, ...) {
  rval <- NULL
  
  if(length(target)[1L] != length(current)[1L]) {
    rval <- c(rval, paste("Length mismatch: target, current differ in number of LHS parts: ",
      length(target)[1L], ", ", length(current)[1L], sep = ""))
  } else if(!isTRUE(all.equal(attr(target, "lhs"), attr(current, "lhs")))) {
    rval <- c(rval, "Formula mismatch: LHS formulas differ in contents")
  }

  if(length(target)[2L] != length(current)[2L]) {
    rval <- c(rval, paste("Length mismatch: target, current differ in number of RHS parts: ",
      length(target)[2L], ", ", length(current)[2L], sep = ""))
  } else if(!isTRUE(all.equal(attr(target, "rhs"), attr(current, "rhs")))) {
    rval <- c(rval, "Formula mismatch: RHS formulas differ in contents")
  }
  
  if(is.null(rval)) TRUE else rval
}

str.Formula <- function(object, ...) {
  le <- length(object)
  ls <- if(sum(le) > 2L | any(le > 1L)) "s" else ""
  writeLines(c(
    sprintf("'Formula' with %s left-hand and %s right-hand side%s: %s",
      le[1L], le[2L], ls, format(object)),
    sprintf("  ..- attr(*, \".Environment\")=%s", format(attr(object, ".Environment")))))
  invisible()
}

## convenience tools #################################################

## split formulas
split_formula <- function(f) {

  stopifnot(inherits(f, "formula"))

  rhs <- if(length(f) > 2) f[[3L]] else f[[2L]]
  lhs <- if(length(f) > 2) f[[2L]] else NULL

  extract_parts <- function(x, sep = "|") {
    if(is.null(x)) return(NULL)
    
    rval <- list()
    if(length(x) > 1L && x[[1L]] == sep) {
      while(length(x) > 1L && x[[1L]] == sep) {
        rval <- c(x[[3L]], rval)
        x <- x[[2L]]
      }
    }
    return(c(x, rval))
  }

  list(lhs = extract_parts(lhs), rhs = extract_parts(rhs))
}

## combine (parts of) formulas
c_formula <- function(f1, f2, sep = "~") {

  stopifnot(length(sep) == 1L, nchar(sep) == 1L,
    sep %in% c("~", "+", "-", "|", "&"))

  if(sep == "~") {
    rval <- . ~ .
    rval[[3L]] <- f2	
    rval[[2L]] <- f1
  } else {
    rval <- as.formula(paste(". ~ .", sep, "."))
    rval[[3L]][[3L]] <- f2
    rval[[3L]][[2L]] <- f1
    rval <- rval[[3L]]
  }

  return(rval)
}

## reassemble formulas
paste_formula <- function(lhs, rhs, lsep = "|", rsep = "|") {

  stopifnot(all(nchar(lsep) == 1L), all(lsep %in% c("+", "|", "&")))
  stopifnot(all(nchar(rsep) == 1L), all(rsep %in% c("+", "|", "&")))
  
  if(length(lhs) > 1L) lsep <- rep(lsep, length.out = length(lhs) - 1L)
  if(length(rhs) > 1L) rsep <- rep(rsep, length.out = length(rhs) - 1L)

  if(is.null(lhs)) lhs <- list()
  if(is.null(rhs)) rhs <- list()
  
  if(!is.list(lhs)) lhs <- list(lhs)
  if(!is.list(rhs)) rhs <- list(rhs)

  lval <- if(length(lhs) > 0L) lhs[[1L]] else NULL
  if(length(lhs) > 1L) {
    for(i in 2L:length(lhs)) lval <- c_formula(lval, lhs[[i]], sep = lsep[[i - 1L]])
  }
  rval <- if(length(rhs) > 0L) rhs[[1L]] else 0 ## FIXME: Is there something better?
  if(length(rhs) > 1L) {
    for(i in 2L:length(rhs)) rval <- c_formula(rval, rhs[[i]], sep = rsep[[i - 1L]])
  }

  c_formula(lval, rval, sep = "~")
}

## simplify a Formula to a formula that can be processed with
## terms/model.frame etc.
simplify_to_formula <- function(Formula, lhs = NULL, rhs = NULL) {

  ## get desired subset as formula and Formula
  form <- formula(Formula, lhs = lhs, rhs = rhs)
  Form <- Formula(form)

  ## convenience functions for checking extended features
  is_lhs_extended <- function(Formula) {
    ## check for multiple parts
    if(length(attr(Formula, "lhs")) > 1L) {
      return(TRUE)
    } else {
    ## and multiple responses
      if(length(attr(Formula, "lhs")) < 1L) return(FALSE)
      return(length(attr(terms(paste_formula(NULL,
        attr(Formula, "lhs"), rsep = "+")), "term.labels")) > 1L)
    }
  }

  is_rhs_extended <- function(Formula) {
    ## check for muliple parts
    length(attr(Formula, "rhs")) > 1L
  }

  ## simplify (if necessary)
  ext_lhs <- is_lhs_extended(Form)
  if(ext_lhs | is_rhs_extended(Form)) {
    form <- if(ext_lhs) {
      if(length(attr(Form, "rhs")) == 1L & identical(attr(Form, "rhs")[[1L]], 0)) {
	paste_formula(NULL, attr(Form, "lhs"), rsep = "+")    
      } else {
        paste_formula(NULL, c(attr(Form, "lhs"), attr(Form, "rhs")), rsep = "+")
      }
    } else {
      paste_formula(attr(Form, "lhs"), attr(Form, "rhs"), rsep = "+")	 
    }
  }

  ## re-attach original environment and return
  environment(form) <- environment(Formula)
  return(form)
}

## check whether formula has a dot (FIXME: can other problems than just '.' occur?)
has_dot <- function(formula) inherits(try(terms(formula), silent = TRUE), "try-error")

kernelwts<-function(X,center,bw,kernel="triangular"){
  dist<-(X-center)/bw
  if(kernel=="triangular"){
    w<-(1-abs(dist))
  } else if (kernel=="rectangular") {
    w<-1/2
  } else if (kernel=="epanechnikov") {
    w<-3/4*(1-dist^2)
  } else if (kernel=="quartic" | kernel=="biweight") {
    w<-15/16*(1-dist^2)^2
  } else if (kernel=="triweight") {
    w<-35/32*(1-dist^2)^3
  } else if (kernel=="tricube") {
    w<-70/81*(1-abs(dist)^3)^3
  } else if (kernel=="gaussian") {
    w<-1/sqrt(2*pi)*exp(-1/2*dist^2)
  } else if (kernel=="cosine") {
    w<-pi/4*cos(pi/2 * dist)
  } else {
    stop("Invalid kernel selection.")
  }
  w<-ifelse(abs(dist)>1&kernel!="gaussian",0,w)
  w<-w/sum(w)
  return(w)
}

IKbandwidth <-function (X,Y,cutpoint=NULL,verbose=FALSE,kernel="triangular") {
  #Implementation of Imbens-Kalyanaraman optimal bandwidth
  # for regression discontinuity
  sub<-complete.cases(X)&complete.cases(Y)
  X <- X[sub]
  Y <- Y[sub]
  Nx<-length(X)
  Ny<-length(Y)
  if(Nx!=Ny)
    stop("Running and outcome variable must be of equal length")
  if(is.null(cutpoint)) {
    cutpoint<-0
    if(verbose) cat("Using default cutpoint of zero.\n")
  } else {
    if(! (typeof(cutpoint) %in% c("integer","double")))
      stop("Cutpoint must be of a numeric type")
  }
  #Now we should be ready to start
  #Pilot bandwidth
  h1<-1.84*sd(X)*Nx^(-1/5)
  left<-X>=(cutpoint-h1) & X<=cutpoint
  right<-X>cutpoint & X<=(cutpoint+h1)
  Nl<-sum(left)
  Nr<-sum(right)
  Ybarl<-mean(Y[left])
  Ybarr<-mean(Y[right])
  fbarx<-(Nl+Nr)/(2*Nx*h1)
  varY<-(sum((Y[left]-Ybarl)^2)+sum((Y[right]-Ybarr)^2))/(Nl+Nr)
  medXl<-median(X[X<=cutpoint])
  medXr<-median(X[X>cutpoint])
  Nl<-sum(X<cutpoint)
  Nr<-sum(X>=cutpoint)
  cX<-X-cutpoint
  if(sum(X[left]>medXl)==0 | sum(X[right]<medXr)==0)
    stop("Insufficient data in vicinity of the cutpoint to calculate bandwidth.")
  #Model a cubic within the pilot bandwidth
  mod<-lm(Y~I(X>=cutpoint)+poly(cX,3,raw=T),subset=(X>=medXl&X<=medXr))
  m3<-6*coef(mod)[5]
  #New bandwidth estimate
  h2l<-3.56*(Nl^(-1/7))*(varY/(fbarx*max(m3^2,0.01)))^(1/7)
  h2r<-3.56*(Nr^(-1/7))*(varY/(fbarx*max(m3^2,0.01)))^(1/7)
  left<-(X>=(cutpoint-h2l)) & (X<cutpoint)
  right<-(X>=cutpoint) & (X<= (cutpoint+h2r))
  Nl<-sum(left)
  Nr<-sum(right)
  if(Nl==0 | Nr==0)
    stop("Insufficient data in vicinity of the cutpoint to calculate bandwidth.")
  #Estimate quadratics for curvature estimation
  mod<-lm(Y~poly(cX,2,raw=T),subset=right)
  m2r<-2*coef(mod)[3]
  mod<-lm(Y~poly(cX,2,raw=T),subset=left)
  m2l<-2*coef(mod)[3]
  rl<-720*varY/(Nl*(h2l^4))
  rr<-720*varY/(Nr*(h2r^4))
  #Which kernel are we using?
  # Method for finding these available in I--K p. 6
  if(kernel=="triangular") {
    ck<-3.43754
  } else if (kernel=="rectangular") {
    ck<-5.40384
  } else if(kernel=="epanechnikov") {
    ck<-3.1999
  } else if(kernel=="quartic" | kernel=="biweight") {
    ck<-3.65362
  } else if(kernel=="triweight") {
    ck<-4.06065
  } else if(kernel=="tricube") {
    ck<-3.68765
  } else if(kernel=="gaussian") {
    ck<-1.25864
  } else if(kernel=="cosine") {
    ck<-3.25869
  } else {
    stop("Unrecognized kernel.") 
  }
  #And there's our optimal bandwidth
  optbw<-ck*(2*varY/(fbarx*((m2r-m2l)^2+rr+rl)))^(1/5)*(Nx^(-1/5))
  left<-(X>=(cutpoint-optbw)) & (X<cutpoint)
  right<-(X>=cutpoint) & (X<= (cutpoint+optbw))
  if(sum(left)==0 | sum(right)==0)
    stop("Insufficient data in the calculated bandwidth.")
  names(optbw)<-NULL
  if(verbose) cat("Imbens-Kalyanamaran Optimal Bandwidth: ",sprintf("%.3f",optbw),"\n")
  return(optbw)
  }


RDestimate<-function(formula, data, subset=NULL, cutpoint=NULL, bw=NULL, kernel="triangular", se.type="HC1", cluster=NULL, verbose=FALSE, model=FALSE, frame=FALSE) {
  call<-match.call()
  if(missing(data)) data<-environment(formula)
  formula<-as.Formula(formula)
  X<-model.frame(formula,rhs=1,lhs=0,data=data,na.action=na.pass)[[1]]
  Y<-model.frame(formula,rhs=0,lhs=NULL,data=data,na.action=na.pass)[[1]]
  if(!is.null(subset)){
    X<-X[subset]
    Y<-Y[subset]
    if(!is.null(cluster)) cluster<-cluster[subset]
  }
  if (!is.null(cluster)) {
    cluster<-as.character(cluster)
    robust.se <- function(model, cluster){
      M <- length(unique(cluster))
      N <- length(cluster)           
      K <- model$rank
      dfc <- (M/(M - 1)) * ((N - 1)/(N - K))
      uj  <- apply(estfun(model), 2, function(x) tapply(x, cluster, sum));
      rcse.cov <- dfc * sandwich(model, meat. = crossprod(uj)/N)
      rcse.se <- coeftest(model, rcse.cov)
      return(rcse.se[2,2])
    }
  }
  na.ok<-complete.cases(X)&complete.cases(Y)
  if(length(all.vars(formula(formula,rhs=1,lhs=F)))>1){
   type<-"fuzzy" 
   Z<-model.frame(formula,rhs=1,lhs=0,data=data,na.action=na.pass)[[2]]
   if(!is.null(subset)) Z<-Z[subset]
   na.ok<-na.ok&complete.cases(Z)
   if(length(all.vars(formula(formula,rhs=1,lhs=F)))>2)
     stop("Invalid formula. Read ?RDestimate for proper syntax")
  } else {
   type="sharp" 
  }
  covs<-NULL
  if(length(formula)[2]>1){
    covs<-model.frame(formula,rhs=2,lhs=0,data=data,na.action=na.pass)
    if(!is.null(subset)) covs<-subset(covs,subset)
    na.ok<-na.ok&complete.cases(covs)
    covs<-subset(covs,na.ok)
  }
  X<-X[na.ok]
  Y<-Y[na.ok]
  if(type=="fuzzy") Z<-as.double(Z[na.ok])
  
  if(is.null(cutpoint)) {
    cutpoint<-0
    if(verbose) cat("No cutpoint provided. Using default cutpoint of zero.\n")
  }
  if(frame) {
    if(type=="sharp") {
      if (!is.null(covs))
        dat.out<-data.frame(X,Y,covs)
      else
        dat.out<-data.frame(X,Y)
    } else {
      if (!is.null(covs))
        dat.out<-data.frame(X,Y,Z,covs)
      else
        dat.out<-data.frame(X,Y,Z)
    }
  }
  if(is.null(bw)) {
    bw<-IKbandwidth(X=X,Y=Y,cutpoint=cutpoint,kernel=kernel, verbose=verbose)
    bws<-c(bw,.5*bw,2*bw)
    names(bws)<-c("LATE","Half-BW","Double-BW")
  } else if (length(bw)==1) {
    bws<-c(bw,.5*bw,2*bw)
    names(bws)<-c("LATE","Half-BW","Double-BW")
  } else {
    bws<-bw
  }
  
  #Setup values to be returned
  o<-list()
  o$type<-type
  o$call<-call
  o$est<-vector(length=length(bws),mode="numeric")
  names(o$est)<-names(bws)
  o$bw<-as.vector(bws)
  o$se<-vector(mode="numeric")
  o$z<-vector(mode="numeric")
  o$p<-vector(mode="numeric")
  o$obs<-vector(mode="numeric")
  o$ci<-matrix(NA,nrow=length(bws),ncol=2)
  o$model<-list()
  if(type=="fuzzy") {
    o$model$firststage<-list()
    o$model$iv<-list()
  }
  o$frame<-list()
  o$na.action<-which(na.ok==FALSE)
  class(o)<-"RD"
  X<-X-cutpoint
  Xl<-(X<0)*X
  Xr<-(X>=0)*X
  Tr<-as.integer(X>=0)
  
  for(bw in bws){
    ibw<-which(bw==bws)
  #Subset to within the bandwidth, except for when using gaussian weighting
    sub<- X>=(-bw) & X<=(+bw)
  
    
  if(kernel=="gaussian") 
    sub<-TRUE

  w<-kernelwts(X,0,bw,kernel=kernel)
  o$obs[ibw]<-sum(w>0)
    
  if(type=="sharp"){
    if(verbose) {
      cat("Running Sharp RD\n")
      cat("Running variable:",all.vars(formula(formula,rhs=1,lhs=F))[1],"\n")
      cat("Outcome variable:",all.vars(formula(formula,rhs=F,lhs=1))[1],"\n")
      if(!is.null(covs)) cat("Covariates:",paste(names(covs),collapse=", "),"\n")
    }
    if(!is.null(covs)) {
      data<-data.frame(Y,Tr,Xl,Xr,covs,w)
      form<-as.formula(paste("Y~Tr+Xl+Xr+",paste(names(covs),collapse="+",sep=""),sep=""))
    } else {
      data<-data.frame(Y,Tr,Xl,Xr,w)
      form<-as.formula(Y~Tr+Xl+Xr)
    }

    mod<-lm(form,weights=w,data=subset(data,w>0))
    if(verbose==TRUE) {
      cat("Model:\n")
      print(summary(mod))
    }
    o$est[ibw]<-coef(mod)["Tr"]
    if (is.null(cluster)) {
      o$se[ibw]<-coeftest(mod,vcovHC(mod,type=se.type))[2,2]
    } else {
      o$se[ibw]<-robust.se(mod,cluster[na.ok][w>0])
    }
    o$z[ibw]<-o$est[ibw]/o$se[ibw]
    o$p[ibw]<-2*pnorm(abs(o$z[ibw]),lower.tail=F)
    o$ci[ibw,]<-c(o$est[ibw]-qnorm(.975)*o$se[ibw],o$est[ibw]+qnorm(.975)*o$se[ibw])

    if(model) o$model[[ibw]]=mod
    if(frame) o$frame[[ibw]]=dat.out

  } else {
    if(verbose){
      cat("Running Fuzzy RD\n")
      #CLEAN UP
      cat("Running variable:",all.vars(formula(formula,rhs=1,lhs=F))[1],"\n")
      cat("Outcome variable:",all.vars(formula(formula,rhs=F,lhs=1))[1],"\n")
      cat("Treatment variable:",all.vars(formula(formula,rhs=1,lhs=F))[2],"\n")
      if(!is.null(covs)) cat("Covariates:",paste(names(covs),collapse=", "),"\n")
    }

    if(!is.null(covs)) {
      data<-data.frame(Y,Tr,Xl,Xr,Z,covs,w)
      form<-as.Formula(paste(
              "Y~Z+Xl+Xr+",paste(names(covs),collapse="+"),
              "|Tr+Xl+Xr+",paste(names(covs),collapse="+"),sep=""))
      form1<-as.Formula(paste("Z~Tr+Xl+Xr+",paste(names(covs),collapse="+",sep="")))
    } else {
      data<-data.frame(Y,Tr,Xl,Xr,Z,w)
      form<-as.Formula(Y~Z+Xl+Xr|Tr+Xl+Xr)
      form1<-as.formula(Z~Tr+Xl+Xr)
    }
    
    mod1<-lm(form1,weights=w,data=subset(data,w>0))
    mod<-ivreg(form,weights=w,data=subset(data,w>0))
    if(verbose==TRUE) {
      cat("First stage:\n")
      print(summary(mod1))
      cat("IV-RD:\n")
      print(summary(mod))
    }
    o$est[ibw]<-coef(mod)["Z"]
    if (is.null(cluster)) {
      o$se[ibw]<-coeftest(mod,vcovHC(mod,type=se.type))[2,2]
    } else {
      o$se[ibw]<-robust.se(mod,cluster[na.ok][w>0])
    }
    o$z[ibw]<-o$est[ibw]/o$se[ibw]
    o$p[ibw]<-2*pnorm(abs(o$z[ibw]),lower.tail=F)
    o$ci[ibw,]<-c(o$est[ibw]-qnorm(.975)*o$se[ibw],o$est[ibw]+qnorm(.975)*o$se[ibw])
    
    if(model) {
      o$model$firststage[[ibw]]<-mod1
      o$model$iv[[ibw]]=mod
    }
    if(frame) o$frame=dat.out
  }
  }
  return(o)
}

summary.RD<-function(object,digits=max(3, getOption("digits") - 3),...){
  cat("\n")
  cat("Call:\n")
  print(object$call)
  cat("\n")
  
  cat("Type:\n")
  cat(object$type,"\n\n")
  
  #If the model wasn't included in the output, we need to get it
  mod<-FALSE
  if("model" %in% names(object$call)) mod<-object$call$model
  if(!mod){
    object$call$model<-TRUE
    object$call$verbose<-FALSE
    object<-eval.parent(object$call)
  }
  n<-length(object$est)

  obs<-vector(length=n)
  if(object$type=="sharp") {
    for(i in 1:n) obs[i]<-length(residuals(object$model[[i]]))
  } else {
    for(i in 1:n) obs[i]<-length(residuals(object$model$iv[[i]]))
  }
  cat("Estimates:\n")
  #Need to get this to give at least as much as stata does in fuzzy designs
  stars<-vector(length=n)

  for(i in 1:n) {
    stars[i]<-if (object$p[i]<0.001) "***" else if(object$p[i]<0.01) "**" else if(object$p[i]<0.05) "*" else if(object$p[i]<0.1) "." else " "
  }

  out<-matrix(
    c(object$bw,
      object$obs,
      object$est,
      object$se,
      object$z,
      object$p),
    nrow=n)
  rownames(out)<-names(object$est)
  colnames(out)<-c("Bandwidth","Observations","Estimate","Std. Error","z value","Pr(>|z|)")

  print.default(cbind(apply(out,2,function(x) format(x,digits=digits)),
                      " "=stars),quote=FALSE,print.gap=2,right=FALSE)
  cat("---\n")
  cat("Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n")
  
  out<-list(coefficients=out)
  class(out)<-"summary.RD"
  fstat<-matrix(NA,nrow=n,ncol=4)
  if(object$type=="sharp") {
    for(i in 1:n) {
      fstat[i,]<-c(summary(object$model[[i]])$fstatistic[[1]],
               summary(object$model[[i]])$fstatistic[[2]],
               summary(object$model[[i]])$fstatistic[[3]],
        pf(summary(object$model[[i]])$fstatistic[[1]],
          summary(object$model[[i]])$fstatistic[[2]],
          summary(object$model[[i]])$fstatistic[[3]])
      )
    }
    fstat[,4]<-2*apply(cbind(fstat[,4],1-fstat[,4]),1,min)
  } else {
    for(i in 1:n) {
      fstat[i,]<-c(
        summary(object$model$iv[[i]])$waldtest[1],
        summary(object$model$iv[[i]])$waldtest[3],
        summary(object$model$iv[[i]])$waldtest[4],
        summary(object$model$iv[[i]])$waldtest[2])
    }
  }
  colnames(fstat)<-c("F","Num. DoF","Denom. DoF","p")
  rownames(fstat)<-names(object$est)
  cat("F-statistics:\n")
  print.default(apply(fstat,2,function(x) format(x,digits=digits)),quote=FALSE,print.gap=2,right=FALSE)
  out$fstat<-fstat
  return(invisible(out))
}


    
```
*** =sample_code
```{r}
# Before running a regression model, let's examine the data
    str(NBA)

# The dataset contains eight variables: a game identifier, a season identifier, two team identifiers (for home and visitor), the quality of the home and visiting teams (a normalized verison of win percentage), and the halftime and end-of-game margins of victory for the home team.

# Create a dummy variable that equals 1 if the home team is ahead at halftime
#---- Question 1-------------------------------------#
      Solution1<-()
      NBA$home.team.winning.at.half <- Solution1
#----------------------------------------------------#
  
# Use the lm function to compute the RD estimate of being ahead at halftime on the final margin of victory. Include the following as additional controls in the regression: home team's halftime margin, home team quality, and away team quality.

#---- Question 2-------------------------------------#
      Solution2<-summary(lm())
#----------------------------------------------------#

# As mentioned in earlier videos, there may be a non-linear effect of the halftime margin on the final margin. In the previous question, we assumed the relationship was linear. In the next question, we will relax this assumption.

# In Question 3, estimate the same regression as in Question 2, but this time include a quadratic effect of halftime margin on the final margin. This is done by adding I(home.team.halftime.margin^2) as an additional term in the model statement of lm.

#---- Question 3-------------------------------------#
      Solution3<-summary(lm())
#----------------------------------------------------#
  
# As a last exercise, let's further relax the quadratic assumption of halftime margin. Instead of estimating an OLS regression using the lm function, we will now use a package called 'rdd' which allows us to examine how sensitive the regression assumptions are for our RD estimate.

# The function we will use is called RDestimate. Without getting into too many details, copy and paste the following syntax as your answer to Question 4: summary(RDestimate(home.team.final.margin ~ home.team.halftime.margin | home.team.qual+away.team.qual,data=NBA))

#---- Question 4-------------------------------------#
      Solution4<-summary(RDestimate(home.team.final.margin ~ home.team.halftime.margin | home.team.qual+away.team.qual,data=NBA))
#----------------------------------------------------#

# Now compare the results of our three causal effect estimates. Which estimate do you think most closely captures the true causal effect of being behind (or ahead) at halftime on the final margin of victory?

```
*** =solution
```{r}
    Solution1<- NBA$home.team.halftime.margin<0
    Solution2<-summary(lm(home.team.final.margin ~ home.team.winning.at.half+home.team.halftime.margin+home.team.qual+away.team.qual,data=NBA))
    Solution3<-summary(lm(home.team.final.margin ~ home.team.winning.at.half+home.team.halftime.margin+I(home.team.halftime.margin^2)+home.team.qual+away.team.qual,data=NBA))
    Solution4<-summary(RDestimate(home.team.final.margin ~ home.team.halftime.margin | home.team.qual+away.team.qual,data=NBA))
```
*** =sct
```{r}
test_object("Solution1")
test_object("Solution2")
test_object("Solution3")
test_object("Solution4")
success_msg("Good work! From the looks of it, the quadratic specification of halftime margin looks very similar to the nonparametric estimate (4.8 points versus 4.4 points). The linear specification seems to not capture the true relationship between halftime margin and final margin, as it only estimates an effect of 2.5 points.")
```


--- type:VideoExercise lang:r aspect_ratio:62.5 xp:50 skills:1 key:5a2a1cae28
## Using RDD to Study Neighborhoods and Schools
*** =video_link
//player.vimeo.com/video/218503634



--- type:MultipleChoiceExercise lang:r xp:50 skills:1 key:da321f7194
## Thinking about confounders in an RD design
Suppose your friend says "I donâ€™t believe the results in Black (1999) because she doesn't control for whether there is a public park nearby the house. Having a park nearby increases the value of a house. Also, districts that have high test scores also tend to have more parks. So this test score effect might just be because of this unobserved confounder!"

*** =instructions
- Agree
- Disagree
- Partly agree
*** =sct
```{r}
msg1 = "Try again"
msg2 = "Correct! It's a reasonable point, but the fact that we do NOT have to observe this confounder is precisely the beauty of RDD! When we look only at houses close to the boundary, those houses are close to each other, by definition. Hence they are both close to the park. So, although the park might affect the value of both houses, it should do so equally. Hence any *difference* in house prices cannot be attributed to the park. It must be attributed to the test score difference."
msg3 = "Try again"
test_mc(correct = 2, feedback_msgs = c(msg1,msg2,msg3))
```



--- type:MultipleChoiceExercise lang:r xp:50 skills:1 key:2781eb8177
## Using geographical borders more generally in RD designs
True or False: As in the Black (1999) paper, anytime there is a geographical border where treatment changes across the border, we can use RDD analysis to learn the causal effect of treatment.

*** =instructions
- True
- False
*** =sct
```{r}
msg1 = "Try again"
msg2 = "Correct! It is true that geographical borders are *often* used to learn about causal effects via RDD. But this requires that the only thing that's different between units on each side of the border is the treatment! This is often not the case. For example, if you look at country borders, there are often many different variables that change as you cross the border, which means it will be hard or impossible to disentangle the effect of just one of these variables."
test_mc(correct = 2, feedback_msgs = c(msg1,msg2))
```

--- type:NormalExercise lang:r aspect_ratio:62.5 xp:50 skills:1 key:1de601c34a
## Examining Manipulation in Regression Discontinuity Designs
This is an example of how to visualize manipulation in RD designs. The main idea with RD is that there should not be any difference in the frequency of observations near the cutoff. If there is, this suggests that observations are likely manipulating the cutoff.

Let's revisit the NBA data we used before. Manipulation in this setting might come about if teams try to be down by 1 or 2 points at halftime (perhaps because they read the research paper published in Management Science!)

*** =instructions
- Using the function `DCdensity` which is already on your workspace, plot the distribution of data near the cutoff
- The statistical test to do this is called the McCrary test. It tests whether there is a discontinuity in the data frequency distribution around the cutoff. A p-value of less than 5% implies that the data are manipulated.
- The syntax for the `DCdensity` function is: `DCdensity(runningvar,discontinuityvalue,plot=TRUE/FALSE)`

*** =pre_exercise_code
```{r}
set.seed(1)
set.seed(1)
n <- 18060

# Initialize dataframe
NBA <- as.data.frame(matrix(0, ncol=10,nrow=n))
colnames(NBA) <- c("id","season","home.team","away.team","home.team.qual","away.team.qual","home.team.halftime.score","away.team.halftime.score","home.team.final.margin","home.team.win")

# Simulate baseline data
NBA$id         <- seq(1,n,1)
NBA$season     <- ifelse(NBA$id %in% seq( 0*1204+1, 1*1204,1),1994,
                  ifelse(NBA$id %in% seq( 1*1204+1, 2*1204,1),1995,
                  ifelse(NBA$id %in% seq( 2*1204+1, 3*1204,1),1996,
                  ifelse(NBA$id %in% seq( 3*1204+1, 4*1204,1),1997,
                  ifelse(NBA$id %in% seq( 4*1204+1, 5*1204,1),1998,
                  ifelse(NBA$id %in% seq( 5*1204+1, 6*1204,1),1999,
                  ifelse(NBA$id %in% seq( 6*1204+1, 7*1204,1),2000,
                  ifelse(NBA$id %in% seq( 7*1204+1, 8*1204,1),2001,
                  ifelse(NBA$id %in% seq( 8*1204+1, 9*1204,1),2002,
                  ifelse(NBA$id %in% seq( 9*1204+1,10*1204,1),2003,
                  ifelse(NBA$id %in% seq(10*1204+1,11*1204,1),2004,
                  ifelse(NBA$id %in% seq(11*1204+1,12*1204,1),2005,
                  ifelse(NBA$id %in% seq(12*1204+1,13*1204,1),2006,
                  ifelse(NBA$id %in% seq(13*1204+1,14*1204,1),2007,
                  ifelse(NBA$id %in% seq(14*1204+1,15*1204,1),2008,0)))))))))))))))
teamqual <- cbind(c("BOS","NJN","NYK","PHL","GSW","LAC","LAL","PHX","SAC","CHI","CLE","DET","IND","MIL","DAL","HOU","MEM","SAS","ATL","CHA","MIA","ORL","WSH","DEN","MIN","SEA","POR","UTA"),
                  matrix(rnorm(28*15,mean=0,sd=1), ncol=15))
NBA$home.team      <- sample(teamqual[,1],n,replace=TRUE)
NBA$away.team      <- sample(teamqual[,1],n,replace=TRUE)
print(paste0('Number of teams playing themselves: ',sum(NBA$home.team==NBA$away.team)/n*100,'%'))
for (i in 1:n) {
    if (NBA$home.team[i]==NBA$away.team[i]) {
        NBA$away.team[i] <- sample(setdiff(teamqual[,1],NBA$home.team[i]),1)
    }
}
print(paste0('Number of teams playing themselves: ',sum(NBA$home.team==NBA$away.team)/n*100,'%'))

# generate team qualities to explain outcomes for each game
for (i in 1:n) {
    NBA$away.team.qual[i] <- as.numeric(teamqual[teamqual[,1]==NBA$away.team[i],NBA$season[i]-1992])
    NBA$home.team.qual[i] <- as.numeric(teamqual[teamqual[,1]==NBA$home.team[i],NBA$season[i]-1992])
}

NBA$home.team <- as.factor(NBA$home.team)
NBA$away.team <- as.factor(NBA$away.team)

# Generate halftime scores
NBA$home.team.halftime.score  <- round(NBA$home.team.qual+rnorm(n, mean=48, sd=4), digits=0)
NBA$away.team.halftime.score  <- round(NBA$away.team.qual+rnorm(n, mean=47, sd=6), digits=0)
NBA$home.team.halftime.margin <- NBA$home.team.halftime.score - NBA$away.team.halftime.score
NBA$home.team.halftime.margin.cheat <- NBA$home.team.halftime.margin-2*(NBA$home.team.halftime.margin==1)-3*(NBA$home.team.halftime.margin==2)

# Remove games that are tied at halftime
print(dim(NBA))
NBA <- NBA[NBA$home.team.halftime.margin!=0,]
print(dim(NBA))
n1 <- dim(NBA)[1]

# Generate final score
NBA$home.team.winning.at.half <- NBA$home.team.halftime.margin>0
NBA$home.team.final.margin    <- round(2 - 5*NBA$home.team.winning.at.half + 2*NBA$home.team.halftime.margin -.05*NBA$home.team.halftime.margin^2 + NBA$home.team.qual - NBA$away.team.qual + rnorm(n1, mean=0, sd=10), digits=0)
NBA$home.team.win             <- NBA$home.team.final.margin>0

# Clean up data
NBA$season <- as.factor(NBA$season)
NBA <- NBA[,c("id","season","home.team","away.team","home.team.qual","away.team.qual","home.team.halftime.margin","home.team.final.margin","home.team.winning.at.half","home.team.win","home.team.halftime.margin.cheat")]


kernelwts<-function(X,center,bw,kernel="triangular"){
  dist<-(X-center)/bw
  if(kernel=="triangular"){
    w<-(1-abs(dist))
  } else if (kernel=="rectangular") {
    w<-1/2
  } else if (kernel=="epanechnikov") {
    w<-3/4*(1-dist^2)
  } else if (kernel=="quartic" | kernel=="biweight") {
    w<-15/16*(1-dist^2)^2
  } else if (kernel=="triweight") {
    w<-35/32*(1-dist^2)^3
  } else if (kernel=="tricube") {
    w<-70/81*(1-abs(dist)^3)^3
  } else if (kernel=="gaussian") {
    w<-1/sqrt(2*pi)*exp(-1/2*dist^2)
  } else if (kernel=="cosine") {
    w<-pi/4*cos(pi/2 * dist)
  } else {
    stop("Invalid kernel selection.")
  }
  w<-ifelse(abs(dist)>1&kernel!="gaussian",0,w)
  w<-w/sum(w)
  return(w)
}


DCdensity <- function(runvar,cutpoint,bin=NULL,bw=NULL,verbose=FALSE,plot=TRUE,ext.out=FALSE,htest=FALSE) {
  runvar <- runvar[complete.cases(runvar)]
  #Grab some summary vars
  rn <- length(runvar)
  rsd <- sd(runvar)
  rmin <- min(runvar)
  rmax <- max(runvar)
  if(missing(cutpoint)) {
    if(verbose) cat("Assuming cutpoint of zero.\n")
    cutpoint<-0
  }
  
  if(cutpoint<=rmin | cutpoint>=rmax){
   stop("Cutpoint must lie within range of runvar") 
  }
  
  if(is.null(bin)) {
    bin <- 2*rsd*rn^(-1/2)
    if(verbose) cat("Using calculated bin size: ",sprintf("%.3f",bin),"\n")
  }
  
  l <- floor((rmin - cutpoint)/bin)*bin + bin/2 + cutpoint #Midpoint of lowest bin
  r <- floor((rmax - cutpoint)/bin)*bin + bin/2 + cutpoint #Midpoint of highest bin
  lc <- cutpoint-(bin/2) #Midpoint of bin just left of breakpoint
  rc <- cutpoint+(bin/2) #Midpoint of bin just right of breakpoint
  j <- floor((rmax - rmin)/bin) + 2
  
  binnum <- round((((floor((runvar - cutpoint)/bin)*bin + bin/2 + cutpoint) - l)/bin) + 1)

  cellval <- rep(0,j)
  for(i in seq(1,rn)){
   cnum <- binnum[i]
   cellval[cnum] <- cellval[cnum]+1
  }
  cellval <- ( cellval / rn ) / bin

  cellmp <- seq(from=1,to=j,by=1)
  cellmp <- floor(((l + (cellmp - 1)*bin ) - cutpoint)/bin)*bin + bin/2 + cutpoint
  
  #If no bandwidth is given, calc it
  if(is.null(bw)){
    #bin number just left of breakpoint
    leftofc <-  round((((floor((lc - cutpoint)/bin)*bin + bin/2 + cutpoint) - l)/bin) + 1) 
    #bin number just right of breakpoint
    rightofc <- round((((floor((rc - cutpoint)/bin)*bin + bin/2 + cutpoint) - l)/bin) + 1)
    if ( rightofc - leftofc != 1) {
      stop("Error occurred in bandwidth calculation")
    }
    cellmpleft <- cellmp[1:leftofc]
    cellmpright <- cellmp[rightofc:j]
    
    #Estimate 4th order polynomial to the left
    P.lm <- lm(
      cellval ~ poly(cellmp,degree=4,raw=T), 
      subset=cellmp<cutpoint
    )
    mse4 <- summary(P.lm)$sigma^2
    lcoef <- coef(P.lm)
    fppleft <- 2*lcoef[3] +
      6*lcoef[4]*cellmpleft + 
      12*lcoef[5]*cellmpleft*cellmpleft
    hleft <- 3.348*(mse4*( cutpoint - l ) / sum(fppleft*fppleft))^(1/5)

    #And to the right
    P.lm <- lm(
      cellval ~ poly(cellmp,degree=4,raw=T), 
      subset=cellmp>=cutpoint
    )
    mse4 <- summary(P.lm)$sigma^2
    rcoef <- coef(P.lm)
    fppright <- 2*rcoef[3] +
      6*rcoef[4]*cellmpright +
      12*rcoef[5]*cellmpright*cellmpright
    hright <- 3.348*(mse4*( r - cutpoint ) / sum(fppright*fppright))^(1/5)


    bw = .5*( hleft + hright )
    if(verbose) cat("Using calculated bandwidth: ",sprintf("%.3f",bw),"\n")
  } 
  if( sum(runvar>cutpoint-bw & runvar<cutpoint) ==0 |
    sum(runvar<cutpoint+bw & runvar>=cutpoint) ==0)
    stop("Insufficient data within the bandwidth.")
  if(plot){
    #estimate density to either side of the cutpoint using a triangular kernel
    d.l<-data.frame(cellmp=cellmp[cellmp<cutpoint],cellval=cellval[cellmp<cutpoint],dist=NA,est=NA,lwr=NA,upr=NA)
    pmin<-cutpoint-2*rsd
    pmax<-cutpoint+2*rsd
    for(i in 1:nrow(d.l)) {
      d.l$dist<-d.l$cellmp-d.l[i,"cellmp"]
      w<-kernelwts(d.l$dist,0,bw,kernel="triangular")
      newd<-data.frame(dist=0)
      pred<-predict(lm(cellval~dist,weights=w,data=d.l),interval="confidence",newdata=newd)
      d.l$est[i]<-pred[1]
      d.l$lwr[i]<-pred[2]
      d.l$upr[i]<-pred[3]
    }
    d.r<-data.frame(cellmp=cellmp[cellmp>=cutpoint],cellval=cellval[cellmp>=cutpoint],dist=NA,est=NA,lwr=NA,upr=NA)
    for(i in 1:nrow(d.r)) {
      d.r$dist<-d.r$cellmp-d.r[i,"cellmp"]
      w<-kernelwts(d.r$dist,0,bw,kernel="triangular")
      newd<-data.frame(dist=0)
      pred<-predict(lm(cellval~dist,weights=w,data=d.r),interval="confidence",newdata=newd)
      d.r$est[i]<-pred[1]
      d.r$lwr[i]<-pred[2]
      d.r$upr[i]<-pred[3]
    }
    #plot to the left
    #return(list(d.l,d.r))
    plot(d.l$cellmp,d.l$est,
       lty=1,lwd=2,col="black",type="l",
       xlim=c(pmin,pmax),
       ylim=c(min(cellval[cellmp<=pmax&cellmp>=pmin]),
              max(cellval[cellmp<=pmax&cellmp>=pmin])),
       xlab=NA,
       ylab=NA,
       main=NA
    )
    
    lines(d.l$cellmp,d.l$lwr,
         lty=2,lwd=1,col="black",type="l"
    )
    lines(d.l$cellmp,d.l$upr,
          lty=2,lwd=1,col="black",type="l"
    )
    
    #plot to the right
    lines(d.r$cellmp,d.r$est,
        lty=1,lwd=2,col="black",type="l"
    )
    lines(d.r$cellmp,d.r$lwr,
          lty=2,lwd=1,col="black",type="l"
    )
    lines(d.r$cellmp,d.r$upr,
          lty=2,lwd=1,col="black",type="l"
    )
    
    #plot the histogram as points
    points(cellmp,cellval,type="p",pch=20)
  }
  cmp<-cellmp
  cval<-cellval
  padzeros <- ceiling(bw/bin)
  jp <- j + 2*padzeros
  if(padzeros>=1) {
    cval <- c(rep(0,padzeros),
               cellval,
               rep(0,padzeros)
             )
    cmp <- c(seq(l-padzeros*bin,l-bin,bin),
              cellmp,
              seq(r+bin,r+padzeros*bin,bin)
            )
  }
  
  #Estimate to the left
  dist <- cmp - cutpoint
  w <- 1-abs(dist/bw)
  w <- ifelse(w>0, w*(cmp<cutpoint), 0)
  w <- (w/sum(w))*jp
  fhatl <- predict(lm(cval~dist,weights=w),newdata=data.frame(dist=0))[[1]]
  
  #Estimate to the right
  w <- 1-abs(dist/bw)
  w <- ifelse(w>0, w*(cmp>=cutpoint), 0)
  w <- (w/sum(w))*jp
  fhatr<-predict(lm(cval~dist,weights=w),newdata=data.frame(dist=0))[[1]]
  
  #Calculate and display dicontinuity estimate
  thetahat <- log(fhatr) - log(fhatl)
  sethetahat <- sqrt( (1/(rn*bw)) * (24/5) * ((1/fhatr) + (1/fhatl)) )
  z<-thetahat/sethetahat
  p<-2*pnorm(abs(z),lower.tail=FALSE)

  if(verbose) {
    cat("Log difference in heights is ",
              sprintf("%.3f",thetahat),
              " with SE ",
              sprintf("%.3f",sethetahat),"\n"
        )
    cat("  this gives a z-stat of ",sprintf("%.3f",z),"\n")
    cat("  and a p value of ",sprintf("%.3f",p),"\n")
  }
  if(ext.out) 
    return(list(theta=thetahat,
                se=sethetahat,
                z=z,
                p=p,
                binsize=bin,
                bw=bw,
                cutpoint=cutpoint,
                data=data.frame(cellmp,cellval)
               )
          )
  else if (htest) {
      # Return an htest object, for compatibility with base R test output.
      structure(list(
          statistic   = c(`z` = z),
          p.value     = p,
          method      = "McCrary (2008) sorting test",
          parameter   = c(`binwidth`  = bin,
                          `bandwidth` = bw,
                          `cutpoint`  = cutpoint),
          alternative = "no apparent sorting"),
          class = "htest")
  }
  else return(p)
}

```

*** =sample_code
```{r}
# Run the McCrary test with the running variable in the NBA data and the cutoff (score tied)
#---- Question 1-------------------------------------#
      Solution1 <- DCdensity(NBA$[running.var],[cutoff],plot=TRUE)
#----------------------------------------------------#
  
# Now issue the command again, but print the p-value output. Put print() around the command
#---- Question 2-------------------------------------#
      Solution2 <- DCdensity(NBA$[running.var],[cutoff],plot=FALSE)
#----------------------------------------------------#

# Now plot using the variable `home.team.halftime.margin.cheat` which has been manipulated
#---- Question 3-------------------------------------#
      Solution3 <- DCdensity(NBA$[new.running.var],[cutoff],plot=TRUE)
#----------------------------------------------------#

# And print its p-value:
#---- Question 4-------------------------------------#
      Solution4 <- DCdensity(NBA$[new.running.var],[cutoff],plot=FALSE)
#----------------------------------------------------#

```
*** =solution
```{r}
    Solution1 <- DCdensity(NBA$home.team.halftime.margin,0,plot=TRUE)
    Solution2 <- print(DCdensity(NBA$home.team.halftime.margin,0,plot=FALSE))
    Solution3 <- DCdensity(NBA$home.team.halftime.margin.cheat,0,plot=TRUE)
    Solution4 <- print(DCdensity(NBA$home.team.halftime.margin.cheat,0,plot=FALSE))
```
*** =sct
```{r}
test_object("Solution1")
test_object("Solution2")
test_object("Solution3")
test_object("Solution4")
success_msg("Good work!")
```


--- type:VideoExercise lang:r aspect_ratio:62.5 xp:50 skills:1 key:f862652b6e
## "Fuzzy" Regression Discontinuity: Addressing Blurry Lines Between Groups
*** =video_link
//player.vimeo.com/video/218504968



--- type:MultipleChoiceExercise lang:r xp:50 skills:1 key:88264adef7
## Identifying fuzzy and sharp RDDs
Consider the following examples. Which is an example of a fuzzy RDD and which is a sharp RDD?

1. Running variable: Age. Treatment: You become eligible for senior citizen welfare programs once you are 65 are older.

2. Running variable: A region's poverty rate. Treatment: Regions become eligible for Federal assistance once their poverty rate is above a certain cutoff.

3. Running variable: Population. Treatment variable: Number of seats a region gets in government (e.g., number of representatives in the U.S. House of Representatives).

*** =instructions
- All are fuzzy
- (1) and (3) are sharp; (2) is fuzzy
- (1) and (2) are sharp; (3) is fuzzy
- (1) is fuzzy; (2) and (3) are sharp
- (1) and (2) are fuzzy; (3) is sharp
- (1) is sharp; (2) and (3) are fuzzy
- (1) and (3) are fuzzy; (2) is sharp
- All are sharp
*** =sct
```{r}
msg1 = "Try again"
msg2 = "Try again"
msg3 = "Try again"
msg4 = "Try again"
msg5 = "Try again"
msg6 = "Try again"
msg7 = "Try again"
msg8 = "Correct! Often the distinction between sharp and fuzzy hinges on the definition of treatment. For (1) and (2), treatment is defined as **eligibility** rather than takeup. A similar idea explains why (3) is sharp: all regions with a certain population have the same number of representatives allocated, and there's no way for a region to send more or fewer representatives. In this way, it's helpful to think of fuzzy RDDs as having a non-compliance problem."
test_mc(correct = 8, feedback_msgs = c(msg1,msg2,msg3,msg4,msg5,msg6,msg7,msg8))
```


--- type:VideoExercise lang:r aspect_ratio:62.5 xp:50 skills:1 key:af9b66df9f
## Fuzzy RDD and Swiss Religion: Checking the Numbers
*** =video_link
//player.vimeo.com/video/218505946



