--- 
title       : "RDD as Causality: Introduction to Regression Discontinuity Design"
description : "This chapter will introduce you to regression discontinuity design (RDD)"
 
# Notes:
#--------------
# Deciding what constitutes "close" to the cutoff
#  - want to increase BW to get more data
#  - want to decrease BW to remove confounding bias
# Nonparametric (local linear) vs. parametric (separate regressions)
# Balance checks (McCrary test, etc.)
# RD as DID
# Sharp vs. Fuzzy: how do you know it's not fuzzy? outcome variable is either 0 or 1


--- type:VideoExercise lang:r aspect_ratio:62.5 xp:50 skills:1 key:efc08cc323
## Regression Discontinuity: Looking at People on the Edge
*** =video_link
//player.vimeo.com/video/218501284


--- type:MultipleChoiceExercise lang:r xp:50 skills:1 key:f401e635a5
## Key variables in an RD design
Which of the following variables is *not* a key (required) variable in a regression discontinuity design?

*** =instructions
- Outcome variable
- Control variable
- Treatment variable
- Running variable
*** =sct
```{r}
msg1 = "The outcome variable is the variable of interest. Try again"
msg2 = "Correct! While most RD's use at least one control variable, doing so is not strictly necessary for RD analysis. (i.e. it is possible, but not common, to run an RD without any control variables.)"
msg3 = "The treatment variable is necessary in order to estimate any kind of causal effect. Try again"
msg4 = "The running variable is how the researcher divides the sample into treatment and control groups. Try again"
test_mc(correct = 2, feedback_msgs = c(msg1,msg2,msg3,msg4))
```

--- type:VideoExercise lang:r aspect_ratio:62.5 xp:50 skills:1 key:79965f29b5
## Regression Discontinuity: More Analysis of Thistlethwaite and Campbell
*** =video_link
//player.vimeo.com/video/218502333


--- type:MultipleChoiceExercise lang:r xp:50 skills:1 key:7977c48f95
## Practice identifying key variables in an RD design
A pair of researchers named Jonah Berger and Devin Pope recently published a study that found that losing can lead to winning. As part of the study, the researchers documented this phenomenon in National Basketball Association (NBA) games. The researchers studied thousands of NBA game outcomes and found that teams who were just behind at halftime were more likely to win the game, after controling for other important factors (like the winning percentage of each team). 

In this study, which variable is the outcome variable, which is the running variable, and which is the treatment variable?

*** =instructions
- The score at halftime is the outcome variable, winning the game is the running variable, and the home team losing at halftime is the treatment variable.
- Winning the game is the outcome variable, the home team losing at halftime is the running variable, and the score at halftime is the treatment variable.
- The score at halftime is the outcome variable, the home team losing at halftime is the running variable, and winning the game is the treatment variable.
- Winning the game is the outcome variable, the score at halftime is the running variable, and the home team losing at halftime is the treatment variable.
*** =sct
```{r}
msg1 = "Try again"
msg2 = "Try again"
msg3 = "Try again"
msg4 = "Correct!"
test_mc(correct = 4, feedback_msgs = c(msg1,msg2,msg3,msg4))
```


--- type:VideoExercise lang:r aspect_ratio:62.5 xp:50 skills:1 key:1a5dfd94ba
## How to Compute Causal Effects in a Regression Discontinuity Analysis
*** =video_link
//player.vimeo.com/video/218502963



--- type:MultipleChoiceExercise lang:r xp:50 skills:1 key:f1b89ac666
## Visualizing RD designs
Most analyses using the RDD approach graphically show the causal effect at the discontinuity. The graph on the right hand part of your screen displays this for the NBA game data mentioned in a previous exercise of this chapter.

Each dot on the plot corresponds to the average margin of victory for the home team taken from a large number of games that had the same halftime margin for the home team. 

As you can see, there is a slight break in the two lines right at a margin of zero, implying that teams who are slightly behind at halftime are slightly more likely (than otherwise expected) to end the game with a positive margin (i.e. win the game).

Why do you think the lines in this graph are sloping upward?

*** =instructions
- Because the RDD isn't controlling for team quality in an appropriate way
- Because we should expect teams that are farther ahead at halftime to have a higher chance of winning, regardless of team quality or any other attribute
- Because there is an inherent home court advantage in the NBA

*** =pre_exercise_code
```{r}
set.seed(1)
n <- 18060

# Initialize dataframe
NBA <- as.data.frame(matrix(0, ncol=10,nrow=n))
colnames(NBA) <- c("id","season","home.team","away.team","home.team.qual","away.team.qual","home.team.halftime.score","away.team.halftime.score","home.team.final.margin","home.team.win")

# Simulate baseline data
NBA$id         <- seq(1,n,1)
NBA$season     <- ifelse(NBA$id %in% seq( 0*1204+1, 1*1204,1),1994,
                  ifelse(NBA$id %in% seq( 1*1204+1, 2*1204,1),1995,
                  ifelse(NBA$id %in% seq( 2*1204+1, 3*1204,1),1996,
                  ifelse(NBA$id %in% seq( 3*1204+1, 4*1204,1),1997,
                  ifelse(NBA$id %in% seq( 4*1204+1, 5*1204,1),1998,
                  ifelse(NBA$id %in% seq( 5*1204+1, 6*1204,1),1999,
                  ifelse(NBA$id %in% seq( 6*1204+1, 7*1204,1),2000,
                  ifelse(NBA$id %in% seq( 7*1204+1, 8*1204,1),2001,
                  ifelse(NBA$id %in% seq( 8*1204+1, 9*1204,1),2002,
                  ifelse(NBA$id %in% seq( 9*1204+1,10*1204,1),2003,
                  ifelse(NBA$id %in% seq(10*1204+1,11*1204,1),2004,
                  ifelse(NBA$id %in% seq(11*1204+1,12*1204,1),2005,
                  ifelse(NBA$id %in% seq(12*1204+1,13*1204,1),2006,
                  ifelse(NBA$id %in% seq(13*1204+1,14*1204,1),2007,
                  ifelse(NBA$id %in% seq(14*1204+1,15*1204,1),2008,0)))))))))))))))
teamqual <- cbind(c("BOS","NJN","NYK","PHL","GSW","LAC","LAL","PHX","SAC","CHI","CLE","DET","IND","MIL","DAL","HOU","MEM","SAS","ATL","CHA","MIA","ORL","WSH","DEN","MIN","SEA","POR","UTA"),
                  matrix(rnorm(28*15,mean=0,sd=1), ncol=15))
NBA$home.team      <- sample(teamqual[,1],n,replace=TRUE)
NBA$away.team      <- sample(teamqual[,1],n,replace=TRUE)
print(paste0('Number of teams playing themselves: ',sum(NBA$home.team==NBA$away.team)/n*100,'%'))
for (i in 1:n) {
    if (NBA$home.team[i]==NBA$away.team[i]) {
        NBA$away.team[i] <- sample(setdiff(teamqual[,1],NBA$home.team[i]),1)
    }
}
print(paste0('Number of teams playing themselves: ',sum(NBA$home.team==NBA$away.team)/n*100,'%'))

# generate team qualities to explain outcomes for each game
for (i in 1:n) {
    NBA$away.team.qual[i] <- as.numeric(teamqual[teamqual[,1]==NBA$away.team[i],NBA$season[i]-1992])
    NBA$home.team.qual[i] <- as.numeric(teamqual[teamqual[,1]==NBA$home.team[i],NBA$season[i]-1992])
}

NBA$home.team <- as.factor(NBA$home.team)
NBA$away.team <- as.factor(NBA$away.team)

# Generate halftime scores
NBA$home.team.halftime.score  <- round(NBA$home.team.qual+rnorm(n, mean=48, sd=4), digits=0)
NBA$away.team.halftime.score  <- round(NBA$away.team.qual+rnorm(n, mean=47, sd=6), digits=0)
NBA$home.team.halftime.margin <- NBA$home.team.halftime.score - NBA$away.team.halftime.score

# Remove games that are tied at halftime
print(dim(NBA))
NBA <- NBA[NBA$home.team.halftime.margin!=0,]
print(dim(NBA))
n1 <- dim(NBA)[1]

# Generate final score
NBA$home.team.winning.at.half <- NBA$home.team.halftime.margin>0
NBA$home.team.final.margin    <- round(2 - 5*NBA$home.team.winning.at.half + 2*NBA$home.team.halftime.margin -.05*NBA$home.team.halftime.margin^2 + NBA$home.team.qual - NBA$away.team.qual + rnorm(n1, mean=0, sd=10), digits=0)
NBA$home.team.win             <- NBA$home.team.final.margin>0

# Clean up data
NBA$season <- as.factor(NBA$season)
NBA <- NBA[,c("id","season","home.team","away.team","home.team.qual","away.team.qual","home.team.halftime.margin","home.team.final.margin","home.team.winning.at.half","home.team.win")]


# Create model
    fit<-lm(home.team.final.margin ~ home.team.winning.at.half + 
         home.team.halftime.margin +
         home.team.qual + away.team.qual, data = NBA)


# Create bins
    bins<-seq(-10,10,(20/15))
    
# Create predicted values
    # Create df with all model values except margins set at 0 or F
        NBA2<-NBA[1:16,]
        NBA2$home.team.winning.at.half<-rep(c(F,T),each=8)
        NBA2$home.team.qual<-0
        NBA2$away.team.qual<-0
        NBA2$home.team.halftime.margin<-bins
    # Predict values
        predicts<- predict(fit, NBA2)

#Turn margins and predicts into df
    df<-data.frame(predicts=predicts, margins=NBA2$home.team.halftime.margin)
        
#create locations of line segments
    mod1<-lm(predicts~margins,data=df[1:8,])
    mod2<-lm(predicts~margins,data=df[9:16,])
    
    y1.1<-mod1$coefficients[1]+mod1$coefficients[2]*-10
    y1.2<-mod1$coefficients[1]+mod1$coefficients[2]*0
    y2.1<-mod2$coefficients[1]+mod2$coefficients[2]*0
    y2.2<-mod2$coefficients[1]+mod2$coefficients[2]*10
  
#plot
    library(ggplot2)
    ggplot(df, aes(x = margins, y = predicts )) + 
      geom_vline(aes(xintercept = 0), color = 'grey', size = 1, linetype = 'dashed') +  # verticle line at discontinuity
      geom_point(size=3) + # adjust point size to your liking
      geom_segment(aes(x = df$margins[1], xend = 0, y = y1.1, yend = y1.2), size = 1)+
      geom_segment(aes(x = 0, xend = df$margins[16], y = y2.1, yend = y2.2), size = 1)

```

*** =sample_code
```{r}
    ggplot(df, aes(x = margins, y = predicts )) + 
      geom_vline(aes(xintercept = 0), color = 'grey', size = 1, linetype = 'dashed') +  # verticle line at discontinuity
      geom_point(size=3) + # adjust point size to your liking
      geom_segment(aes(x = df$margins[1], xend = 0, y = y1.1, yend = y1.2), size = 1)+
      geom_segment(aes(x = 0, xend = df$margins[16], y = y2.1, yend = y2.2), size = 1)+
      labs(x = "Margin of victory")+
      labs(y = "Probability of winning")
```
*** =sct
```{r}
msg1 = "Almost, try again."
msg2 = "Good job! This is an example of why conclusions from RDD analyses are only valid at the boundary of the discontinuity! If we get too far away, we will be picking up effects that will confound our causal inference."
msg3 = "Almost, try again."
test_mc(correct = 2, feedback_msgs = c(msg1,msg2,msg3))
```


--- type:NormalExercise lang:r aspect_ratio:62.5 xp:50 skills:1 key:8dfb92fd8e
## Practice Computing Regression Discontinuity Effects
Let's continue with the NBA research example. On your workspace is a simulated data set, called `NBA`, that closely resembles the data analyzed by Berger and Pope. The data frame `NBA` contains game characteristics for over 18,000 NBA games between 1994 and 2009.

The outcome variable we are interested in is called `home.team.final.margin`, which is the final margin of victory (or loss) for the home team. The running variable in this RDD is called `home.team.halftime.margin`, or the margin of victory (or loss) for the home team at halftime. We define treatment as the home team being ahead at halftime, i.e. `home.team.halftime.margin > 0`. (You can also define it as the home team being behind at halftime and you will get the same results, just opposite sign.)

In this exercise, you will compute the treatment effect of being ahead at halftime on the final margin of victory. You will use regression methods as well as nonparametric methods to assess how robust the effect is.

*** =instructions
- Use OLS regression to estimate the treatment effect of being behind at halftime on the final margin of victory under two different parametric scenarios
- Estimate the treatment effect using non-parametric methods

*** =pre_exercise_code
```{r}
# require(rdd)

set.seed(1)
n <- 18060

# Initialize dataframe
NBA <- as.data.frame(matrix(0, ncol=10,nrow=n))
colnames(NBA) <- c("id","season","home.team","away.team","home.team.qual","away.team.qual","home.team.halftime.score","away.team.halftime.score","home.team.final.margin","home.team.win")

# Simulate baseline data
NBA$id         <- seq(1,n,1)
NBA$season     <- ifelse(NBA$id %in% seq( 0*1204+1, 1*1204,1),1994,
                  ifelse(NBA$id %in% seq( 1*1204+1, 2*1204,1),1995,
                  ifelse(NBA$id %in% seq( 2*1204+1, 3*1204,1),1996,
                  ifelse(NBA$id %in% seq( 3*1204+1, 4*1204,1),1997,
                  ifelse(NBA$id %in% seq( 4*1204+1, 5*1204,1),1998,
                  ifelse(NBA$id %in% seq( 5*1204+1, 6*1204,1),1999,
                  ifelse(NBA$id %in% seq( 6*1204+1, 7*1204,1),2000,
                  ifelse(NBA$id %in% seq( 7*1204+1, 8*1204,1),2001,
                  ifelse(NBA$id %in% seq( 8*1204+1, 9*1204,1),2002,
                  ifelse(NBA$id %in% seq( 9*1204+1,10*1204,1),2003,
                  ifelse(NBA$id %in% seq(10*1204+1,11*1204,1),2004,
                  ifelse(NBA$id %in% seq(11*1204+1,12*1204,1),2005,
                  ifelse(NBA$id %in% seq(12*1204+1,13*1204,1),2006,
                  ifelse(NBA$id %in% seq(13*1204+1,14*1204,1),2007,
                  ifelse(NBA$id %in% seq(14*1204+1,15*1204,1),2008,0)))))))))))))))
teamqual <- cbind(c("BOS","NJN","NYK","PHL","GSW","LAC","LAL","PHX","SAC","CHI","CLE","DET","IND","MIL","DAL","HOU","MEM","SAS","ATL","CHA","MIA","ORL","WSH","DEN","MIN","SEA","POR","UTA"),
                  matrix(rnorm(28*15,mean=0,sd=1), ncol=15))
NBA$home.team      <- sample(teamqual[,1],n,replace=TRUE)
NBA$away.team      <- sample(teamqual[,1],n,replace=TRUE)
print(paste0('Number of teams playing themselves: ',sum(NBA$home.team==NBA$away.team)/n*100,'%'))
for (i in 1:n) {
    if (NBA$home.team[i]==NBA$away.team[i]) {
        NBA$away.team[i] <- sample(setdiff(teamqual[,1],NBA$home.team[i]),1)
    }
}
print(paste0('Number of teams playing themselves: ',sum(NBA$home.team==NBA$away.team)/n*100,'%'))

# generate team qualities to explain outcomes for each game
for (i in 1:n) {
    NBA$away.team.qual[i] <- as.numeric(teamqual[teamqual[,1]==NBA$away.team[i],NBA$season[i]-1992])
    NBA$home.team.qual[i] <- as.numeric(teamqual[teamqual[,1]==NBA$home.team[i],NBA$season[i]-1992])
}

NBA$home.team <- as.factor(NBA$home.team)
NBA$away.team <- as.factor(NBA$away.team)

# Generate halftime scores
NBA$home.team.halftime.score  <- round(NBA$home.team.qual+rnorm(n, mean=48, sd=4), digits=0)
NBA$away.team.halftime.score  <- round(NBA$away.team.qual+rnorm(n, mean=47, sd=6), digits=0)
NBA$home.team.halftime.margin <- NBA$home.team.halftime.score - NBA$away.team.halftime.score

# Remove games that are tied at halftime
print(dim(NBA))
NBA <- NBA[NBA$home.team.halftime.margin!=0,]
print(dim(NBA))
n1 <- dim(NBA)[1]

# Generate final score
NBA$home.team.winning.at.half <- NBA$home.team.halftime.margin>0
NBA$home.team.final.margin    <- round(2 - 5*NBA$home.team.winning.at.half + 2*NBA$home.team.halftime.margin -.05*NBA$home.team.halftime.margin^2 + NBA$home.team.qual - NBA$away.team.qual + rnorm(n1, mean=0, sd=10), digits=0)
NBA$home.team.win             <- NBA$home.team.final.margin>0

# Clean up data
NBA$season <- as.factor(NBA$season)
NBA <- NBA[,c("id","season","home.team","away.team","home.team.qual","away.team.qual","home.team.halftime.margin","home.team.final.margin","home.team.winning.at.half","home.team.win")]

    
```
*** =sample_code
```{r}
# Before running a regression model, let's examine the data
    str(NBA)

# The dataset contains eight variables: a game identifier, a season identifier, two team identifiers (for home and visitor), the quality of the home and visiting teams (a normalized verison of win percentage), and the halftime and end-of-game margins of victory for the home team.

# Create a dummy variable that equals 1 if the home team is ahead at halftime
#---- Question 1-------------------------------------#
      Solution1<-()
      NBA$home.team.winning.at.half <- Solution1
#----------------------------------------------------#
  
# Use the lm function to compute the RD estimate of being ahead at halftime on the final margin of victory. Include the following as additional controls in the regression: home team's halftime margin, home team quality, and away team quality.

#---- Question 2-------------------------------------#
      Solution2<-summary(lm())
#----------------------------------------------------#

# As mentioned in earlier videos, there may be a non-linear effect of the halftime margin on the final margin. In the previous question, we assumed the relationship was linear. In the next question, we will relax this assumption.

# In Question 3, estimate the same regression as in Question 2, but this time include a quadratic effect of halftime margin on the final margin. This is done by adding I(home.team.halftime.margin^2) as an additional term in the model statement of lm.

#---- Question 3-------------------------------------#
      Solution3<-summary(lm())
#----------------------------------------------------#
  
# As a last exercise, let's further relax the quadratic assumption of halftime margin. Instead of estimating an OLS regression using the lm function, we will now use a package called 'rdd' which allows us to examine how sensitive the regression assumptions are for our RD estimate.

# The function we will use is called RDestimate. Without getting into too many details, copy and paste the following syntax as your answer to Question 4: summary(RDestimate(home.team.final.margin ~ home.team.halftime.margin | home.team.qual+away.team.qual,data=NBA))


# Now compare the results of our three causal effect estimates. Which estimate do you think most closely captures the true causal effect of being behind (or ahead) at halftime on the final margin of victory?

```
*** =solution
```{r}
    Solution1<- NBA$home.team.halftime.margin<0
    Solution2<-summary(lm(home.team.final.margin ~ home.team.winning.at.half+home.team.halftime.margin+home.team.qual+away.team.qual,data=NBA))
    Solution3<-summary(lm(home.team.final.margin ~ home.team.winning.at.half+home.team.halftime.margin+I(home.team.halftime.margin^2)+home.team.qual+away.team.qual,data=NBA))
```
*** =sct
```{r}
test_object("Solution1")
test_object("Solution2")
test_object("Solution3")
success_msg("Good work! From the looks of it, the quadratic specification of halftime margin looks very similar to the nonparametric estimate (4.8 points versus 4.4 points). The linear specification seems to not capture the true relationship between halftime margin and final margin, as it only estimates an effect of 2.5 points.")
```


--- type:VideoExercise lang:r aspect_ratio:62.5 xp:50 skills:1 key:5a2a1cae28
## Using RDD to Study Neighborhoods and Schools
*** =video_link
//player.vimeo.com/video/218503634



--- type:MultipleChoiceExercise lang:r xp:50 skills:1 key:da321f7194
## Thinking about confounders in an RD design
Suppose your friend says "I donâ€™t believe the results in Black (1999) because she doesn't control for whether there is a public park nearby the house. Having a park nearby increases the value of a house. Also, districts that have high test scores also tend to have more parks. So this test score effect might just be because of this unobserved confounder!"

*** =instructions
- Agree
- Disagree
- Partly agree
*** =sct
```{r}
msg1 = "Try again"
msg2 = "Correct! It's a reasonable point, but the fact that we do NOT have to observe this confounder is precisely the beauty of RDD! When we look only at houses close to the boundary, those houses are close to each other, by definition. Hence they are both close to the park. So, although the park might affect the value of both houses, it should do so equally. Hence any *difference* in house prices cannot be attributed to the park. It must be attributed to the test score difference."
msg3 = "Try again"
test_mc(correct = 2, feedback_msgs = c(msg1,msg2,msg3))
```



--- type:MultipleChoiceExercise lang:r xp:50 skills:1 key:2781eb8177
## Using geographical borders more generally in RD designs
True or False: As in the Black (1999) paper, anytime there is a geographical border where treatment changes across the border, we can use RDD analysis to learn the causal effect of treatment.

*** =instructions
- True
- False
*** =sct
```{r}
msg1 = "Try again"
msg2 = "Correct! It is true that geographical borders are *often* used to learn about causal effects via RDD. But this requires that the only thing that's different between units on each side of the border is the treatment! This is often not the case. For example, if you look at country borders, there are often many different variables that change as you cross the border, which means it will be hard or impossible to disentangle the effect of just one of these variables."
test_mc(correct = 2, feedback_msgs = c(msg1,msg2))
```

--- type:NormalExercise lang:r aspect_ratio:62.5 xp:50 skills:1 key:1de601c34a
## Examining Manipulation in Regression Discontinuity Designs
This is an example of how to visualize manipulation in RD designs. The main idea with RD is that there should not be any difference in the frequency of observations near the cutoff. If there is, this suggests that observations are likely manipulating the cutoff. This question examines this idea.

*** =instructions
- Check out this code

*** =pre_exercise_code
```{r}
set.seed(1)
n <- 10000

DCdensity <- function(runvar,cutpoint,bin=NULL,bw=NULL,verbose=FALSE,plot=TRUE,ext.out=FALSE,htest=FALSE) {
  runvar <- runvar[complete.cases(runvar)]
  #Grab some summary vars
  rn <- length(runvar)
  rsd <- sd(runvar)
  rmin <- min(runvar)
  rmax <- max(runvar)
  if(missing(cutpoint)) {
    if(verbose) cat("Assuming cutpoint of zero.\n")
    cutpoint<-0
  }
  
  if(cutpoint<=rmin | cutpoint>=rmax){
   stop("Cutpoint must lie within range of runvar") 
  }
  
  if(is.null(bin)) {
    bin <- 2*rsd*rn^(-1/2)
    if(verbose) cat("Using calculated bin size: ",sprintf("%.3f",bin),"\n")
  }
  
  l <- floor((rmin - cutpoint)/bin)*bin + bin/2 + cutpoint #Midpoint of lowest bin
  r <- floor((rmax - cutpoint)/bin)*bin + bin/2 + cutpoint #Midpoint of highest bin
  lc <- cutpoint-(bin/2) #Midpoint of bin just left of breakpoint
  rc <- cutpoint+(bin/2) #Midpoint of bin just right of breakpoint
  j <- floor((rmax - rmin)/bin) + 2
  
  binnum <- round((((floor((runvar - cutpoint)/bin)*bin + bin/2 + cutpoint) - l)/bin) + 1)

  cellval <- rep(0,j)
  for(i in seq(1,rn)){
   cnum <- binnum[i]
   cellval[cnum] <- cellval[cnum]+1
  }
  cellval <- ( cellval / rn ) / bin

  cellmp <- seq(from=1,to=j,by=1)
  cellmp <- floor(((l + (cellmp - 1)*bin ) - cutpoint)/bin)*bin + bin/2 + cutpoint
  
  #If no bandwidth is given, calc it
  if(is.null(bw)){
    #bin number just left of breakpoint
    leftofc <-  round((((floor((lc - cutpoint)/bin)*bin + bin/2 + cutpoint) - l)/bin) + 1) 
    #bin number just right of breakpoint
    rightofc <- round((((floor((rc - cutpoint)/bin)*bin + bin/2 + cutpoint) - l)/bin) + 1)
    if ( rightofc - leftofc != 1) {
      stop("Error occurred in bandwidth calculation")
    }
    cellmpleft <- cellmp[1:leftofc]
    cellmpright <- cellmp[rightofc:j]
    
    #Estimate 4th order polynomial to the left
    P.lm <- lm(
      cellval ~ poly(cellmp,degree=4,raw=T), 
      subset=cellmp<cutpoint
    )
    mse4 <- summary(P.lm)$sigma^2
    lcoef <- coef(P.lm)
    fppleft <- 2*lcoef[3] +
      6*lcoef[4]*cellmpleft + 
      12*lcoef[5]*cellmpleft*cellmpleft
    hleft <- 3.348*(mse4*( cutpoint - l ) / sum(fppleft*fppleft))^(1/5)

    #And to the right
    P.lm <- lm(
      cellval ~ poly(cellmp,degree=4,raw=T), 
      subset=cellmp>=cutpoint
    )
    mse4 <- summary(P.lm)$sigma^2
    rcoef <- coef(P.lm)
    fppright <- 2*rcoef[3] +
      6*rcoef[4]*cellmpright +
      12*rcoef[5]*cellmpright*cellmpright
    hright <- 3.348*(mse4*( r - cutpoint ) / sum(fppright*fppright))^(1/5)


    bw = .5*( hleft + hright )
    if(verbose) cat("Using calculated bandwidth: ",sprintf("%.3f",bw),"\n")
  } 
  if( sum(runvar>cutpoint-bw & runvar<cutpoint) ==0 |
    sum(runvar<cutpoint+bw & runvar>=cutpoint) ==0)
    stop("Insufficient data within the bandwidth.")
  if(plot){
    #estimate density to either side of the cutpoint using a triangular kernel
    d.l<-data.frame(cellmp=cellmp[cellmp<cutpoint],cellval=cellval[cellmp<cutpoint],dist=NA,est=NA,lwr=NA,upr=NA)
    pmin<-cutpoint-2*rsd
    pmax<-cutpoint+2*rsd
    for(i in 1:nrow(d.l)) {
      d.l$dist<-d.l$cellmp-d.l[i,"cellmp"]
      w<-kernelwts(d.l$dist,0,bw,kernel="triangular")
      newd<-data.frame(dist=0)
      pred<-predict(lm(cellval~dist,weights=w,data=d.l),interval="confidence",newdata=newd)
      d.l$est[i]<-pred[1]
      d.l$lwr[i]<-pred[2]
      d.l$upr[i]<-pred[3]
    }
    d.r<-data.frame(cellmp=cellmp[cellmp>=cutpoint],cellval=cellval[cellmp>=cutpoint],dist=NA,est=NA,lwr=NA,upr=NA)
    for(i in 1:nrow(d.r)) {
      d.r$dist<-d.r$cellmp-d.r[i,"cellmp"]
      w<-kernelwts(d.r$dist,0,bw,kernel="triangular")
      newd<-data.frame(dist=0)
      pred<-predict(lm(cellval~dist,weights=w,data=d.r),interval="confidence",newdata=newd)
      d.r$est[i]<-pred[1]
      d.r$lwr[i]<-pred[2]
      d.r$upr[i]<-pred[3]
    }
    #plot to the left
    #return(list(d.l,d.r))
    plot(d.l$cellmp,d.l$est,
       lty=1,lwd=2,col="black",type="l",
       xlim=c(pmin,pmax),
       ylim=c(min(cellval[cellmp<=pmax&cellmp>=pmin]),
              max(cellval[cellmp<=pmax&cellmp>=pmin])),
       xlab=NA,
       ylab=NA,
       main=NA
    )
    
    lines(d.l$cellmp,d.l$lwr,
         lty=2,lwd=1,col="black",type="l"
    )
    lines(d.l$cellmp,d.l$upr,
          lty=2,lwd=1,col="black",type="l"
    )
    
    #plot to the right
    lines(d.r$cellmp,d.r$est,
        lty=1,lwd=2,col="black",type="l"
    )
    lines(d.r$cellmp,d.r$lwr,
          lty=2,lwd=1,col="black",type="l"
    )
    lines(d.r$cellmp,d.r$upr,
          lty=2,lwd=1,col="black",type="l"
    )
    
    #plot the histogram as points
    points(cellmp,cellval,type="p",pch=20)
  }
  cmp<-cellmp
  cval<-cellval
  padzeros <- ceiling(bw/bin)
  jp <- j + 2*padzeros
  if(padzeros>=1) {
    cval <- c(rep(0,padzeros),
               cellval,
               rep(0,padzeros)
             )
    cmp <- c(seq(l-padzeros*bin,l-bin,bin),
              cellmp,
              seq(r+bin,r+padzeros*bin,bin)
            )
  }
  
  #Estimate to the left
  dist <- cmp - cutpoint
  w <- 1-abs(dist/bw)
  w <- ifelse(w>0, w*(cmp<cutpoint), 0)
  w <- (w/sum(w))*jp
  fhatl <- predict(lm(cval~dist,weights=w),newdata=data.frame(dist=0))[[1]]
  
  #Estimate to the right
  w <- 1-abs(dist/bw)
  w <- ifelse(w>0, w*(cmp>=cutpoint), 0)
  w <- (w/sum(w))*jp
  fhatr<-predict(lm(cval~dist,weights=w),newdata=data.frame(dist=0))[[1]]
  
  #Calculate and display dicontinuity estimate
  thetahat <- log(fhatr) - log(fhatl)
  sethetahat <- sqrt( (1/(rn*bw)) * (24/5) * ((1/fhatr) + (1/fhatl)) )
  z<-thetahat/sethetahat
  p<-2*pnorm(abs(z),lower.tail=FALSE)

  if(verbose) {
    cat("Log difference in heights is ",
              sprintf("%.3f",thetahat),
              " with SE ",
              sprintf("%.3f",sethetahat),"\n"
        )
    cat("  this gives a z-stat of ",sprintf("%.3f",z),"\n")
    cat("  and a p value of ",sprintf("%.3f",p),"\n")
  }
  if(ext.out) 
    return(list(theta=thetahat,
                se=sethetahat,
                z=z,
                p=p,
                binsize=bin,
                bw=bw,
                cutpoint=cutpoint,
                data=data.frame(cellmp,cellval)
               )
          )
  else if (htest) {
      # Return an htest object, for compatibility with base R test output.
      structure(list(
          statistic   = c(`z` = z),
          p.value     = p,
          method      = "McCrary (2008) sorting test",
          parameter   = c(`binwidth`  = bin,
                          `bandwidth` = bw,
                          `cutpoint`  = cutpoint),
          alternative = "no apparent sorting"),
          class = "htest")
  }
  else return(p)
}

# Generate some data with no discontinuity.
xNoDisc<-runif(n,-1,1)
xDisc<-runif(n,-1,1)
xDisc<-xDisc+2*(runif(n,-1,1)>0&x<0)
```

*** =sample_code
```{r}

# Test with data that has no discontinuity
#---- Question 1-------------------------------------#
      Solution1 <- DCdensity(xNoDisc,0,plot=FALSE)
#----------------------------------------------------#
  
# Now test with some data with a discontinuity.
#---- Question 2-------------------------------------#
      Solution2<-DCdensity(xDisc,0,plot=FALSE)
#----------------------------------------------------#

```
*** =solution
```{r}
    Solution1 <- DCdensity(xNoDisc,0,plot=FALSE)
    Solution2 <- DCdensity(xDisc,0,plot=FALSE)
```
*** =sct
```{r}
test_object("Solution1")
test_object("Solution2")
success_msg("Good work!")
```


--- type:VideoExercise lang:r aspect_ratio:62.5 xp:50 skills:1 key:f862652b6e
## "Fuzzy" Regression Discontinuity: Addressing Blurry Lines Between Groups
*** =video_link
//player.vimeo.com/video/218504968



--- type:MultipleChoiceExercise lang:r xp:50 skills:1 key:88264adef7
## Identifying fuzzy and sharp RDDs
Consider the following examples. Which is an example of a fuzzy RDD and which is a sharp RDD?

1. Running variable: Age. Treatment: You become eligible for senior citizen welfare programs once you are 65 are older.

2. Running variable: A region's poverty rate. Treatment: Regions become eligible for Federal assistance once their poverty rate is above a certain cutoff.

3. Running variable: Population. Treatment variable: Number of seats a region gets in government (e.g., number of representatives in the U.S. House of Representatives).

*** =instructions
- All are fuzzy
- (1) and (3) are sharp; (2) is fuzzy
- (1) and (2) are sharp; (3) is fuzzy
- (1) is fuzzy; (2) and (3) are sharp
- (1) and (2) are fuzzy; (3) is sharp
- (1) is sharp; (2) and (3) are fuzzy
- (1) and (3) are fuzzy; (2) is sharp
- All are sharp
*** =sct
```{r}
msg1 = "Try again"
msg2 = "Try again"
msg3 = "Try again"
msg4 = "Try again"
msg5 = "Try again"
msg6 = "Try again"
msg7 = "Try again"
msg8 = "Correct! Often the distinction between sharp and fuzzy hinges on the definition of treatment. For (1) and (2), treatment is defined as **eligibility** rather than takeup. A similar idea explains why (3) is sharp: all regions with a certain population have the same number of representatives allocated, and there's no way for a region to send more or fewer representatives. In this way, it's helpful to think of fuzzy RDDs as having a non-compliance problem."
test_mc(correct = 8, feedback_msgs = c(msg1,msg2,msg3,msg4,msg5,msg6,msg7,msg8))
```


--- type:VideoExercise lang:r aspect_ratio:62.5 xp:50 skills:1 key:af9b66df9f
## Fuzzy RDD and Swiss Religion: Checking the Numbers
*** =video_link
//player.vimeo.com/video/218505946



